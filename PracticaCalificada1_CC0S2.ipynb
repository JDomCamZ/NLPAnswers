{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e35a97d0",
      "metadata": {
        "id": "e35a97d0"
      },
      "source": [
        "## Primera práctica calificada\n",
        "\n",
        "**Nombre y Apellidos: Juan Jose Camarena Zamalloa**\n",
        "\n",
        "**Código: 20171342H**\n",
        "\n",
        "### Reglas para la Prueba Calificada\n",
        "\n",
        "- Queda terminantemente prohibido el uso de herramientas como ChatGPT, WhatsApp, o cualquier herramienta similar durante la realización de esta prueba. El uso de estas herramientas, por cualquier motivo, resultará en la anulación inmediata de la evaluación.\n",
        "\n",
        "- Las respuestas deben presentarse con una explicación detallada, utilizando términos técnicos apropiados. La mera descripción sin el uso de terminología técnica, especialmente términos discutidos en clase, se considerará insuficiente y podrá resultar en que la respuesta sea marcada como incorrecta.\n",
        "\n",
        "\n",
        "- Cada estudiante debe presentar su propio trabajo. Los códigos iguales o muy parecidos entre sí serán considerados como una violación a la integridad académica, implicando una copia, y serán sancionados de acuerdo con las políticas de la universidad.\n",
        "\n",
        "- Todos los estudiantes deben subir sus repositorios de código a la plataforma del curso, según las instrucciones proporcionadas. La fecha y hora de la última actualización del repositorio serán consideradas como la hora de entrega.\n",
        "\n",
        "- La claridad, orden, y presentación general de las evaluaciones serán tomadas en cuenta en la calificación final. Se espera un nivel de profesionalismo en la documentación y presentación del código y las respuestas escritas.\n",
        "\n",
        "\n",
        "#### Instrucciones de entrega para la prueba calificada\n",
        "\n",
        "- Presenta la dirección de tu repositorio personal donde se encuentre este cuaderno con tus respuestas desarrolladas.\n",
        "- Todo cambio fuera de la hora y fecha del examen realizado dentro del repositorio no se tomará en cuenta y se procederá a anular la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6884a0",
      "metadata": {
        "id": "3c6884a0"
      },
      "source": [
        "1 . Dividir un bloque de texto en subunidades significativas es una parte esencial del procesamiento de texto. El texto se puede dividir en caracteres o palabras individuales o en algún punto intermedio. A continuación se muestra un enfoque muy básico que divide el texto utilizando espacios en blanco. Ya existe un defecto, ya que la última palabra `dog` tiene puntuación.\n",
        "\n",
        "```\n",
        "'The quick brown fox jumps over the lazy dog.'.split(' ')\n",
        "```\n",
        "\n",
        "Con los modelos Transformer, realizamos tokenizaciones de subpalabras y dividimos el texto mediante un tokenizador prediseñado. Esto ha sido entrenado en una gran cantidad de texto donde ha aprendido cuáles son palabras comunes y cuáles son menos comunes y podrían dividirse en partes (que a menudo parecen sílabas).\n",
        "\n",
        "Primero, carguemos uno para un modelo de Transformer común `distilgpt2`. Podemos cargarlo con el siguiente código. El modelo distilgpt2 es un modelo más pequeño basado en gpt2 que vimos en clase, que es un predecesor del modelo de lenguaje que sustenta ChatGPT.\n",
        "\n",
        "```\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "```\n",
        "\n",
        "El tokenizador tiene una función `tokenizer.tokenize`  que divide el texto, así como funciones como `tokenizer.vocab`, `tokenizer.encode`, `tokenizer.decode`. El tokenizador tiene muchos parámetros para brindar control adicional. Por ejemplo, a veces es necesario truncar cadenas muy largas (ya que existe un límite en la longitud de entrada a los modelos Transformer). Puedes utilizar la función `tokenizer.encode` para tokenizar una oración como \"Kelvingrove is a beautiful park in Glasgow.\" que puedes recortar a solo 5 tokens usando `truncation=True` y `max_length=5`.\n",
        "\n",
        "\n",
        "Revisa: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "* Encuentra una palabra en inglés que sea tokenizada en 3, 4, 5 o incluso 6 subtokens con el tokenizador distilgpt2 (1 punto)\n",
        "* Escribe una función en Python  para tokenizar un texto y mostrar los tokens y sus IDs, también escribe una función que utilice el tokenizador `datificate/gpt2-small-spanish` para identificar palabras que se descomponen en 3, 4, 5 o 6 subtokens, lo cual es interesante para entender la granularidad del tokenizador (2 puntos).\n",
        "\n",
        "* Al tokenizar, utilizaremos el tokenizador con el parámetro `return_tensors='pt'`. Esto coloca los datos en el formato de un tensor PyTorch, que se utiliza como entrada para un modelo Transformer. PyTorch es una biblioteca comúnmente utilizada para el aprendizaje profundo y HuggingFace se basa en ella. No usaremos PyTorch directamente. Tokeniza: `\"A horse! a horse! my kingdom for a\"` (1 punto)\n",
        "\n",
        "* Implementar un script en Python que use AutoModelForCausalLM para cargar un modelo de lenguaje causal. El ejercicio consistirá en tokenizar un texto, pasarlo a un modelo Transformer, y luego analizar las probabilidades de los tokens generados para identificar el más probable. Revisa: https://huggingface.co/docs/transformers/tasks/language_modeling (3 puntos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b98310",
      "metadata": {
        "id": "a9b98310",
        "outputId": "533ff7ee-ed0c-49a6-f821-b4159a7949d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6470b6",
      "metadata": {
        "id": "1c6470b6",
        "outputId": "cbc8a84e-d10e-4b4b-f477-6db1a172ea25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 417, 1075, 305, 303]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tokenizer.encode(\"Kelvingrove is a beautiful park in Glasgow\", truncation=True, max_length=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ee3de9",
      "metadata": {
        "id": "48ee3de9",
        "outputId": "defe510f-e589-4dfc-dbb2-c88427d2f721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 417, 1075, 305, 303, 318, 257, 4950, 3952, 287, 23995]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer.encode(\"Kelvingrove is a beautiful park in Glasgow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f843fa",
      "metadata": {
        "id": "72f843fa",
        "outputId": "9e374fcc-764d-4b59-a3ed-dac89839753f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['K',\n",
              " 'el',\n",
              " 'ving',\n",
              " 'ro',\n",
              " 've',\n",
              " 'Ġis',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġpark',\n",
              " 'Ġin',\n",
              " 'ĠGlasgow']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Kelvingrove is a beautiful park in Glasgow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7af8ca6",
      "metadata": {
        "id": "b7af8ca6",
        "outputId": "8a32c4e1-0e5d-4d6d-a35d-9f6891c1a32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['K', 'el', 'ving', 'ro', 've']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Kelvingrove\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aede7cb4",
      "metadata": {
        "id": "aede7cb4"
      },
      "source": [
        "1. Palabra en inglés tokenizada en distintos subtokens, este caso 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b595b721",
      "metadata": {
        "id": "b595b721",
        "outputId": "16e4f0a8-14ef-4ec8-cd8c-a47197febb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['con', 'vers', 'ational', 'ists']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.tokenize(\"conversationalists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70df7ec8",
      "metadata": {
        "id": "70df7ec8"
      },
      "source": [
        "2. Se escriben las siguientes funciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d08d7f3",
      "metadata": {
        "id": "0d08d7f3"
      },
      "outputs": [],
      "source": [
        "# Funcion para tokenizar una oración en subtokens con sus IDs respectivos\n",
        "\n",
        "def tokenizer_ids(word):\n",
        "    tokenized_words = tokenizer.tokenize(word)\n",
        "    tokenized_IDs = tokenizer.encode(word)\n",
        "    for i, token in enumerate(tokenized_words):\n",
        "        print(f\"- {token}: {tokenized_IDs[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6c971b",
      "metadata": {
        "id": "3e6c971b",
        "outputId": "5038d63d-725c-4694-bc5b-834a5ce6c35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- K: 42\n",
            "- el: 417\n",
            "- ving: 1075\n",
            "- ro: 305\n",
            "- ve: 303\n",
            "- Ġis: 318\n",
            "- Ġa: 257\n",
            "- Ġbeautiful: 4950\n",
            "- Ġpark: 3952\n",
            "- Ġin: 287\n",
            "- ĠGlasgow: 23995\n"
          ]
        }
      ],
      "source": [
        "tokenizer_ids(\"Kelvingrove is a beautiful park in Glasgow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a542817",
      "metadata": {
        "id": "2a542817"
      },
      "outputs": [],
      "source": [
        "tokenizer_esp = AutoTokenizer.from_pretrained('datificate/gpt2-small-spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd01a990",
      "metadata": {
        "id": "bd01a990"
      },
      "outputs": [],
      "source": [
        "# Funcion para identificar si una palabra usando el tokenizador datificate/gpt2-small-spanish se obtiene una palabra con más de 2 subtokens y menos de 7 subtokens\n",
        "\n",
        "def check_tokens(word):\n",
        "    word_tokenized = tokenizer_esp.tokenize(word)\n",
        "    if len(word_tokenized) > 2 and len(word_tokenized) < 7:\n",
        "        print(f\"{word} es una palabra que se subdivide en más de 2 subtokens y menos de 7 subtokens.\")\n",
        "    else:\n",
        "        print(f\"{word} no es una palabra que se subdivide en más de 2 subtokens y menos de 7 subtokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c17b4c",
      "metadata": {
        "id": "78c17b4c",
        "outputId": "b8665582-fe30-4a5a-ed32-e9d08c551889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perro no es una palabra que se subdivide en más de 2 subtokens y menos de 7 subtokens.\n"
          ]
        }
      ],
      "source": [
        "check_tokens(\"perro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9442575",
      "metadata": {
        "id": "c9442575",
        "outputId": "e4524dd6-2114-4fab-9575-2df08d2377d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "humongosaurio es una palabra que se subdivide en más de 2 subtokens y menos de 7 subtokens.\n"
          ]
        }
      ],
      "source": [
        "check_tokens(\"humongosaurio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ccaf1e",
      "metadata": {
        "id": "53ccaf1e"
      },
      "source": [
        "3. Tokenizamos con el parámetro `return_tensors='pt'`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cec7425",
      "metadata": {
        "id": "0cec7425",
        "outputId": "2ff025ab-be2d-44a7-f0f3-c7766808b666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'Ġhorse', '!', 'Ġa', 'Ġhorse', '!', 'Ġmy', 'Ġkingdom', 'Ġfor', 'Ġa']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenizer.tokenize(\"A horse! a horse! my kingdom for a\",return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5836febd",
      "metadata": {
        "id": "5836febd"
      },
      "source": [
        "4. Realizando el script de Python que utilice AutoModelForCausalLM:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "HvTt9bC8dQWO"
      },
      "id": "HvTt9bC8dQWO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Somatic hypermutation allows the immune system to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "9QUElzxhhd5D"
      },
      "id": "9QUElzxhhd5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yufTGYL5hLCp",
        "outputId": "4fee5d9d-78a6-42d4-d905-c0c355641fb2"
      },
      "id": "yufTGYL5hLCp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   50, 13730,  8718,    76,  7094,  3578,   262, 10900,  1080,   284,\n",
              "          6105,  3031,   284,   584, 10685, 25973,    13,  1081,   257,  1255,\n",
              "            11,   262, 10900,  1080,   318,  1498,   284,  1620,   287,    12,\n",
              "           290,   503,    12,  1659,    12,  2618,  5499,   326,   423,   587,\n",
              "          8018,   329,  1811,   812,   416,   584, 10685,  3640,   287,   428,\n",
              "          2214,    13,   383,  7035, 26256,   326,  8718,    76,  7094, 13536,\n",
              "           262, 10900,  1080,   284,  3938,  6324,   284,   584, 10685, 25973,\n",
              "            11,   290,   340,   743,  2620,   703,   262, 10900,  1080,  7767,\n",
              "           262,  3288, 25973,   290,   262,  2858,    13,   770,  1429,   318,\n",
              "          3058,   852,  4855,   287,   636,   416,   262,  1578,  1829,  2351,\n",
              "         15523,  5136,    13,   198,   198,   198,   198,   198,   198,   198]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH99XcobhpVT",
        "outputId": "c4fc2c27-ab9d-4701-b37a-54c18ca439af"
      },
      "id": "DH99XcobhpVT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Somatic hypermutation allows the immune system to properly respond to other biological stimuli. As a result, the immune system is able to perform in- and out-of-body functions that have been recognized for several years by other biological studies in this field. The authors speculate that hypermutation enables the immune system to fully react to other biological stimuli, and it may increase how the immune system processes the natural stimuli and the environment. This process is currently being supported in part by the United States National Cancer Institute.\\n\\n\\n\\n\\n\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e017711b",
      "metadata": {
        "id": "e017711b"
      },
      "source": [
        "2 . En estos ejercicios se trata de profundizar en los modelos CBOW y Skim-Gram visto en clase:\n",
        "\n",
        "* Implementa un algoritmo para descubrir todos los 2-skip-2-gramas de una oración dada (2 puntos)\n",
        "* Expresa la función de pérdida únicamente como función de las entradas y los pesos, después de eliminar las variables de la capa oculta. (1 punto)\n",
        "\n",
        "* Calcula los gradientes de la función de pérdida con respecto a los pesos en las capas de entrada y salida (2 puntos)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Algortimo para descubrir lo pedido:"
      ],
      "metadata": {
        "id": "O-BnUU23kkJx"
      },
      "id": "O-BnUU23kkJx"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "GsRWEiakkhj6"
      },
      "id": "GsRWEiakkhj6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3469828f",
      "metadata": {
        "id": "3469828f"
      },
      "outputs": [],
      "source": [
        "def two_skip_two_grams_sentence(word):\n",
        "  skip_grams = []\n",
        "  tokens = nlp.tokenizer(word)\n",
        "  cont = 0\n",
        "  for i in range(len(tokens)-3):\n",
        "    skip_gram = tokens[i], tokens[i+3]\n",
        "    skip_grams.append(skip_gram)\n",
        "\n",
        "  return skip_grams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "two_skip_two_grams_sentence(\"this is a sentence with a lot of words that it's answer has every word with 2 skips as a pair\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWWrejrul2Q_",
        "outputId": "ba3ce853-011d-46e6-e1fa-318cb0686125"
      },
      "id": "CWWrejrul2Q_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(this, sentence),\n",
              " (is, with),\n",
              " (a, a),\n",
              " (sentence, lot),\n",
              " (with, of),\n",
              " (a, words),\n",
              " (lot, that),\n",
              " (of, it),\n",
              " (words, 's),\n",
              " (that, answer),\n",
              " (it, has),\n",
              " ('s, every),\n",
              " (answer, word),\n",
              " (has, with),\n",
              " (every, 2),\n",
              " (word, skips),\n",
              " (with, as),\n",
              " (2, a),\n",
              " (skips, pair)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b0c6e6",
      "metadata": {
        "id": "37b0c6e6"
      },
      "source": [
        "3 . De la actividad de modelos de lenguaje realizada en clase, las técnicas de suavizado, como el suavizado de Laplace, a menudo se agregan a los modelos de lenguaje de n-gramas para manejar probabilidades de 0. ¿Cómo podrías modificar tu código para incluirlo?.\n",
        "Puedes tambien experimentar con el suavizado de Good-Turing que ajusta las cuentas de los n-gramas basándose en la frecuencia de frecuencias de n-gramas. Es especialmente útil para redistribuir la masa de probabilidad a n-gramas no observados (2 puntos)\n",
        "\n",
        "De la misma actividad en los modelos que exploramos anteriormente, utilizamos suavizado. ¿Qué sucede con los cálculos de perplejidad cuando no se aplica el suavizado?  A veces se utiliza el suavizado interpolado o de \"back-off\" en los modelos de lenguaje de n-gramas. Esta técnica calcula la probabilidad promedio ponderada de modelos con diferentes valores de `n`. Implementa esto. ¿Cómo afecta esto la perplejidad en el conjunto de pruebas retenido? ¿Qué pasa con la perplejidad sobre los datos de entrenamiento? (3 puntos)\n",
        "\n",
        "\n",
        "El término 'Naïve' en la clasificación por Naïve Bayes se refiere a la suposición de independencia e idéntica distribución. Extiende el clasificador Naïve Bayes utilizando el concepto de modelado de lenguaje bigrama. El nuevo modelo pierde el atributo 'Naïve'. ¿Puedes integrar características de bolsa de palabras en este modelo utilizando técnicas de suavizado? (3 puntos)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0559b314",
      "metadata": {
        "id": "0559b314"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c302556f",
      "metadata": {
        "id": "c302556f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}