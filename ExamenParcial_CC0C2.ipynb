{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e35a97d0",
      "metadata": {
        "id": "e35a97d0"
      },
      "source": [
        "## Examen Parcial CC0C2\n",
        "\n",
        "**Nombre y Apellidos: Juan José Camarena Zamalloa**\n",
        "\n",
        "**Código: 20171342H**\n",
        "\n",
        "### Reglas para el Examen Parcial\n",
        "\n",
        "- Queda terminantemente prohibido el uso de herramientas como ChatGPT, WhatsApp, o cualquier herramienta similar durante la realización de esta prueba. El uso de estas herramientas, por cualquier motivo, resultará en la anulación inmediata de la evaluación. Puedes utilizar los cuadernos y datos alojados en github.\n",
        "\n",
        "- Las respuestas deben presentarse con una explicación detallada, utilizando términos técnicos apropiados. La mera descripción sin el uso de terminología técnica, especialmente términos discutidos en clase, se considerará insuficiente y podrá resultar en que la respuesta sea marcada como incorrecta.\n",
        "\n",
        "- Cada estudiante debe presentar su propio trabajo. Los códigos iguales o muy parecidos entre sí serán considerados como una violación a la integridad académica, implicando una copia, y serán sancionados de acuerdo con las políticas de la universidad.\n",
        "\n",
        "- Todos los estudiantes deben subir sus repositorios de código a la plataforma del curso, según las instrucciones proporcionadas. La fecha y hora de la última actualización del repositorio serán consideradas como la hora de entrega.\n",
        "\n",
        "- La claridad, orden, y presentación general de las evaluaciones serán tomadas en cuenta en la calificación final. Se espera un nivel de profesionalismo en la documentación y presentación del código y las respuestas escritas.\n",
        "\n",
        "\n",
        "#### Instrucciones de entrega para la prueba calificada\n",
        "\n",
        "- Presenta la dirección de tu repositorio personal donde se encuentre este cuaderno con tus respuestas desarrolladas.\n",
        "- Todo cambio fuera de la hora y fecha del examen realizado dentro del repositorio no se tomará en cuenta y se procederá a anular la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a55574d-3519-42cb-bd20-357ab7b6abcd",
      "metadata": {
        "id": "1a55574d-3519-42cb-bd20-357ab7b6abcd"
      },
      "source": [
        "### Problema 1\n",
        "\n",
        "El subsampling en el contexto de los modelos de Word2Vec es una técnica utilizada para reducir el número de veces que se entrenan palabras muy frecuentes. Se basa en la idea de que las palabras extremadamente comunes (como preposiciones y conjunciones) proporcionan menos información de contexto valiosa en comparación con las palabras menos frecuentes. En la práctica, cada palabra en el conjunto de entrenamiento tiene una probabilidad calculada de ser \"saltada\" durante el entrenamiento, dependiendo de su frecuencia. Esto ayuda a acelerar el entrenamiento y a mejorar la calidad de las representaciones de palabras menos frecuentes, que podrían verse oscurecidas por palabras de alta frecuencia.\n",
        "\n",
        "El negative sampling es una técnica de optimización para reducir la complejidad computacional de actualizar los pesos en la red neuronal en modelos como Word2Vec. En lugar de actualizar los pesos de todas las palabras del vocabulario para cada ejemplo de entrenamiento (lo cual es muy costoso computacionalmente), el negative sampling actualiza solo un pequeño número de \"palabras negativas\" (ejemplos negativos seleccionados aleatoriamente) junto con la palabra objetivo (ejemplo positivo). Esto no solo acelera significativamente el entrenamiento sino que también mejora la calidad de las representaciones vectoriales al enfocarse en distinguir la palabra objetivo de un pequeño subconjunto de palabras negativas.\n",
        "\n",
        "La correlación de Spearman es una medida estadística que evalúa la fuerza y la dirección de la asociación entre dos variables clasificadas. A diferencia de la correlación de Pearson, que requiere que las variables sean de escala intervalo o de razón y aproximadamente normales, la correlación de Spearman no hace suposiciones sobre la distribución de los datos y se basa en rangos. Es especialmente útil en el contexto de Word2Vec cuando se evalúa cómo las similitudes coseno calculadas entre vectores de palabras se comparan con juicios humanos de similitud (usualmente dados en estudios donde las personas califican qué tan similares son las palabras). Al correlacionar estos dos conjuntos de rankings (el calculado y el humano), se puede obtener una medida de cuán bien el modelo captura relaciones semánticas que coinciden con las percepciones humanas.\n",
        "\n",
        "#### Ejercicios:\n",
        "\n",
        "1. Implementa los modelos CBOW y Skip-gram en Python sin utilizar bibliotecas de alto nivel como Gensim (2 puntos).\n",
        "    - Escribe el código para inicializar los pesos de la red, realizar el entrenamiento mediante descenso de gradiente y calcular la función de pérdida.\n",
        "    - Añade mecanismos de subsampling y negative sampling para mejorar la eficiencia del entrenamiento.\n",
        "2. Analiza cómo diferentes hiperparámetros afectan la calidad de los embeddings vectoriales (2 puntos).\n",
        "    - Entrena modelos Word2Vec con diferentes tamaños de ventana, dimensiones de vector y tasas de aprendizaje. Utiliza un conjunto de datos estándar como el corpus de texto de Wikipedia.\n",
        "    - Evalúa los modelos usando tareas de analogía de palabras y calcula la correlación de Spearman entre las similitudes humanas y las  calculadas por el modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo:"
      ],
      "metadata": {
        "id": "1gKU4aJ9uAlY"
      },
      "id": "1gKU4aJ9uAlY"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c21a4488-65b4-4c7b-8f76-d072a5cc976a",
      "metadata": {
        "id": "c21a4488-65b4-4c7b-8f76-d072a5cc976a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "class word2vec(object):\n",
        "    def __init__(self):\n",
        "        self.N = 10\n",
        "        self.X_train = []\n",
        "        self.y_train = []\n",
        "        self.window_size = 2\n",
        "        self.alpha = 0.001\n",
        "        self.words = []\n",
        "        self.word_index = {}\n",
        "\n",
        "    def initialize(self,V,data):\n",
        "        self.V = V\n",
        "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\n",
        "        self.words = data\n",
        "        for i in range(len(data)):\n",
        "            self.word_index[data[i]] = i\n",
        "\n",
        "\n",
        "    def feed_forward(self,X):\n",
        "        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "        self.u = np.dot(self.W1.T,self.h)\n",
        "        #print(self.u)\n",
        "        self.y = softmax(self.u)\n",
        "        return self.y\n",
        "\n",
        "    def backpropagate(self,x,t):\n",
        "        e = self.y - np.asarray(t).reshape(self.V,1)\n",
        "        # e.shape is V x 1\n",
        "        dLdW1 = np.dot(self.h,e.T)\n",
        "        X = np.array(x).reshape(self.V,1)\n",
        "        dLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "        self.W1 = self.W1 - self.alpha*dLdW1\n",
        "        self.W = self.W - self.alpha*dLdW\n",
        "\n",
        "    def train(self,epochs):\n",
        "        for x in range(1,epochs):\n",
        "            self.loss = 0\n",
        "            for j in range(len(self.X_train)):\n",
        "                self.feed_forward(self.X_train[j])\n",
        "                self.backpropagate(self.X_train[j],self.y_train[j])\n",
        "                C = 0\n",
        "                for m in range(self.V):\n",
        "                    if(self.y_train[j][m]):\n",
        "                        self.loss += -1*self.u[m][0]\n",
        "                        C += 1\n",
        "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "            print(\"epoch \",x, \" loss = \",self.loss)\n",
        "            self.alpha *= 1/( (1+self.alpha*x) )\n",
        "\n",
        "    def predict(self,word,number_of_predictions):\n",
        "        if word in self.words:\n",
        "            index = self.word_index[word]\n",
        "            X = [0 for i in range(self.V)]\n",
        "            X[index] = 1\n",
        "            prediction = self.feed_forward(X)\n",
        "            output = {}\n",
        "            for i in range(self.V):\n",
        "                output[prediction[i][0]] = i\n",
        "\n",
        "            top_context_words = []\n",
        "            for k in sorted(output,reverse=True):\n",
        "                top_context_words.append(self.words[output[k]])\n",
        "                if(len(top_context_words)>=number_of_predictions):\n",
        "                    break\n",
        "\n",
        "            return top_context_words\n",
        "        else:\n",
        "            print(\"Word not found in dictionary\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocessing(corpus):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    training_data = []\n",
        "    sentences = corpus.split(\".\")\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].strip()\n",
        "        sentence = sentences[i].split()\n",
        "        x = [word.strip(string.punctuation) for word in sentence\n",
        "                                     if word not in stop_words]\n",
        "        x = [word.lower() for word in x]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "\n",
        "    #for i in range(len(words)):\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "\n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "\n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "metadata": {
        "id": "V5_YHmY4kCeI"
      },
      "id": "V5_YHmY4kCeI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\n",
        "corpus += \"The earth revolves around the sun. The moon revolves around the earth\"\n",
        "epochs = 1000\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n",
        "\n",
        "print(w2v.predict(\"around\",3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lO-_-GFkFZ9",
        "outputId": "d6b26124-0223-4e4f-9e77-8eaa14f29299"
      },
      "id": "8lO-_-GFkFZ9",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  41.40161488251882\n",
            "epoch  2  loss =  41.33491393373918\n",
            "epoch  3  loss =  41.268722969649346\n",
            "epoch  4  loss =  41.20310167307102\n",
            "epoch  5  loss =  41.13810730625654\n",
            "epoch  6  loss =  41.07379441660124\n",
            "epoch  7  loss =  41.01021457497016\n",
            "epoch  8  loss =  40.9474161495398\n",
            "epoch  9  loss =  40.88544411724018\n",
            "epoch  10  loss =  40.82433991405069\n",
            "epoch  11  loss =  40.76414132457327\n",
            "epoch  12  loss =  40.704882410516106\n",
            "epoch  13  loss =  40.64659347698554\n",
            "epoch  14  loss =  40.58930107483027\n",
            "epoch  15  loss =  40.53302803671992\n",
            "epoch  16  loss =  40.477793544183314\n",
            "epoch  17  loss =  40.42361322248152\n",
            "epoch  18  loss =  40.37049925995194\n",
            "epoch  19  loss =  40.31846054832127\n",
            "epoch  20  loss =  40.26750284044784\n",
            "epoch  21  loss =  40.217628921998816\n",
            "epoch  22  loss =  40.16883879369062\n",
            "epoch  23  loss =  40.12112986090252\n",
            "epoch  24  loss =  40.07449712770483\n",
            "epoch  25  loss =  40.02893339260808\n",
            "epoch  26  loss =  39.98442944362798\n",
            "epoch  27  loss =  39.94097425056132\n",
            "epoch  28  loss =  39.89855515266864\n",
            "epoch  29  loss =  39.85715804025682\n",
            "epoch  30  loss =  39.816767528936225\n",
            "epoch  31  loss =  39.77736712559372\n",
            "epoch  32  loss =  39.738939385366706\n",
            "epoch  33  loss =  39.70146605912385\n",
            "epoch  34  loss =  39.66492823115486\n",
            "epoch  35  loss =  39.629306446942216\n",
            "epoch  36  loss =  39.59458083103488\n",
            "epoch  37  loss =  39.560731195166916\n",
            "epoch  38  loss =  39.52773713686523\n",
            "epoch  39  loss =  39.49557812887207\n",
            "epoch  40  loss =  39.464233599770395\n",
            "epoch  41  loss =  39.43368300624691\n",
            "epoch  42  loss =  39.40390589746029\n",
            "epoch  43  loss =  39.37488197200178\n",
            "epoch  44  loss =  39.34659112794557\n",
            "epoch  45  loss =  39.31901350648699\n",
            "epoch  46  loss =  39.29212952966104\n",
            "epoch  47  loss =  39.265919932621344\n",
            "epoch  48  loss =  39.240365790944125\n",
            "epoch  49  loss =  39.21544854340159\n",
            "epoch  50  loss =  39.1911500106276\n",
            "epoch  51  loss =  39.16745241007464\n",
            "epoch  52  loss =  39.14433836763651\n",
            "epoch  53  loss =  39.121790926286195\n",
            "epoch  54  loss =  39.09979355205357\n",
            "epoch  55  loss =  39.07833013764298\n",
            "epoch  56  loss =  39.05738500396647\n",
            "epoch  57  loss =  39.03694289984637\n",
            "epoch  58  loss =  39.016989000118045\n",
            "epoch  59  loss =  38.997508902343505\n",
            "epoch  60  loss =  38.97848862232677\n",
            "epoch  61  loss =  38.95991458860356\n",
            "epoch  62  loss =  38.94177363606079\n",
            "epoch  63  loss =  38.9240529988259\n",
            "epoch  64  loss =  38.90674030255086\n",
            "epoch  65  loss =  38.889823556202934\n",
            "epoch  66  loss =  38.87329114346113\n",
            "epoch  67  loss =  38.85713181380717\n",
            "epoch  68  loss =  38.841334673388396\n",
            "epoch  69  loss =  38.82588917572145\n",
            "epoch  70  loss =  38.810785112297154\n",
            "epoch  71  loss =  38.79601260313869\n",
            "epoch  72  loss =  38.781562087359404\n",
            "epoch  73  loss =  38.76742431375929\n",
            "epoch  74  loss =  38.75359033149464\n",
            "epoch  75  loss =  38.74005148084951\n",
            "epoch  76  loss =  38.726799384134\n",
            "epoch  77  loss =  38.713825936729926\n",
            "epoch  78  loss =  38.701123298301034\n",
            "epoch  79  loss =  38.68868388418203\n",
            "epoch  80  loss =  38.676500356957646\n",
            "epoch  81  loss =  38.664565618240886\n",
            "epoch  82  loss =  38.65287280065698\n",
            "epoch  83  loss =  38.64141526003841\n",
            "epoch  84  loss =  38.63018656783393\n",
            "epoch  85  loss =  38.61918050373393\n",
            "epoch  86  loss =  38.608391048512374\n",
            "epoch  87  loss =  38.59781237708547\n",
            "epoch  88  loss =  38.58743885178531\n",
            "epoch  89  loss =  38.577265015846805\n",
            "epoch  90  loss =  38.56728558710506\n",
            "epoch  91  loss =  38.55749545189989\n",
            "epoch  92  loss =  38.547889659183966\n",
            "epoch  93  loss =  38.53846341483026\n",
            "epoch  94  loss =  38.529212076134584\n",
            "epoch  95  loss =  38.520131146508675\n",
            "epoch  96  loss =  38.51121627035863\n",
            "epoch  97  loss =  38.50246322814426\n",
            "epoch  98  loss =  38.49386793161389\n",
            "epoch  99  loss =  38.485426419209865\n",
            "epoch  100  loss =  38.47713485163924\n",
            "epoch  101  loss =  38.46898950760502\n",
            "epoch  102  loss =  38.46098677969238\n",
            "epoch  103  loss =  38.45312317040513\n",
            "epoch  104  loss =  38.44539528834738\n",
            "epoch  105  loss =  38.43779984454531\n",
            "epoch  106  loss =  38.430333648904515\n",
            "epoch  107  loss =  38.42299360679795\n",
            "epoch  108  loss =  38.4157767157799\n",
            "epoch  109  loss =  38.40868006242173\n",
            "epoch  110  loss =  38.40170081926444\n",
            "epoch  111  loss =  38.39483624188449\n",
            "epoch  112  loss =  38.38808366606801\n",
            "epoch  113  loss =  38.38144050508989\n",
            "epoch  114  loss =  38.37490424709375\n",
            "epoch  115  loss =  38.36847245256853\n",
            "epoch  116  loss =  38.362142751918924\n",
            "epoch  117  loss =  38.35591284312519\n",
            "epoch  118  loss =  38.34978048948951\n",
            "epoch  119  loss =  38.343743517465505\n",
            "epoch  120  loss =  38.33779981456744\n",
            "epoch  121  loss =  38.3319473273564\n",
            "epoch  122  loss =  38.326184059500356\n",
            "epoch  123  loss =  38.32050806990508\n",
            "epoch  124  loss =  38.31491747091348\n",
            "epoch  125  loss =  38.30941042657049\n",
            "epoch  126  loss =  38.30398515095105\n",
            "epoch  127  loss =  38.29863990654882\n",
            "epoch  128  loss =  38.29337300272301\n",
            "epoch  129  loss =  38.28818279420147\n",
            "epoch  130  loss =  38.28306767963742\n",
            "epoch  131  loss =  38.27802610021817\n",
            "epoch  132  loss =  38.2730565383234\n",
            "epoch  133  loss =  38.26815751623157\n",
            "epoch  134  loss =  38.263327594871996\n",
            "epoch  135  loss =  38.25856537262146\n",
            "epoch  136  loss =  38.25386948414314\n",
            "epoch  137  loss =  38.24923859926657\n",
            "epoch  138  loss =  38.244671421906816\n",
            "epoch  139  loss =  38.240166689021514\n",
            "epoch  140  loss =  38.235723169604235\n",
            "epoch  141  loss =  38.2313396637129\n",
            "epoch  142  loss =  38.22701500153168\n",
            "epoch  143  loss =  38.2227480424655\n",
            "epoch  144  loss =  38.2185376742654\n",
            "epoch  145  loss =  38.2143828121841\n",
            "epoch  146  loss =  38.21028239816014\n",
            "epoch  147  loss =  38.20623540003002\n",
            "epoch  148  loss =  38.202240810766824\n",
            "epoch  149  loss =  38.19829764774458\n",
            "epoch  150  loss =  38.19440495202745\n",
            "epoch  151  loss =  38.190561787682626\n",
            "epoch  152  loss =  38.186767241116236\n",
            "epoch  153  loss =  38.18302042043117\n",
            "epoch  154  loss =  38.17932045480633\n",
            "epoch  155  loss =  38.17566649389634\n",
            "epoch  156  loss =  38.17205770725084\n",
            "epoch  157  loss =  38.16849328375291\n",
            "epoch  158  loss =  38.164972431075796\n",
            "epoch  159  loss =  38.161494375157176\n",
            "epoch  160  loss =  38.15805835969055\n",
            "epoch  161  loss =  38.154663645632894\n",
            "epoch  162  loss =  38.15130951072825\n",
            "epoch  163  loss =  38.147995249046396\n",
            "epoch  164  loss =  38.144720170536274\n",
            "epoch  165  loss =  38.141483600593574\n",
            "epoch  166  loss =  38.138284879642\n",
            "epoch  167  loss =  38.135123362727654\n",
            "epoch  168  loss =  38.13199841912603\n",
            "epoch  169  loss =  38.128909431961524\n",
            "epoch  170  loss =  38.125855797838604\n",
            "epoch  171  loss =  38.12283692648429\n",
            "epoch  172  loss =  38.11985224040188\n",
            "epoch  173  loss =  38.11690117453519\n",
            "epoch  174  loss =  38.113983175943\n",
            "epoch  175  loss =  38.111097703483544\n",
            "epoch  176  loss =  38.10824422750853\n",
            "epoch  177  loss =  38.10542222956624\n",
            "epoch  178  loss =  38.10263120211391\n",
            "epoch  179  loss =  38.09987064823831\n",
            "epoch  180  loss =  38.09714008138502\n",
            "epoch  181  loss =  38.09443902509552\n",
            "epoch  182  loss =  38.09176701275219\n",
            "epoch  183  loss =  38.089123587330775\n",
            "epoch  184  loss =  38.08650830116011\n",
            "epoch  185  loss =  38.083920715688905\n",
            "epoch  186  loss =  38.081360401259296\n",
            "epoch  187  loss =  38.078826936887054\n",
            "epoch  188  loss =  38.07631991004794\n",
            "epoch  189  loss =  38.073838916470464\n",
            "epoch  190  loss =  38.07138355993439\n",
            "epoch  191  loss =  38.068953452075085\n",
            "epoch  192  loss =  38.06654821219345\n",
            "epoch  193  loss =  38.06416746707114\n",
            "epoch  194  loss =  38.061810850791105\n",
            "epoch  195  loss =  38.05947800456308\n",
            "epoch  196  loss =  38.05716857655405\n",
            "epoch  197  loss =  38.054882221723375\n",
            "epoch  198  loss =  38.05261860166254\n",
            "epoch  199  loss =  38.05037738443927\n",
            "epoch  200  loss =  38.048158244446036\n",
            "epoch  201  loss =  38.045960862252606\n",
            "epoch  202  loss =  38.04378492446276\n",
            "epoch  203  loss =  38.04163012357479\n",
            "epoch  204  loss =  38.03949615784584\n",
            "epoch  205  loss =  38.037382731159894\n",
            "epoch  206  loss =  38.03528955289932\n",
            "epoch  207  loss =  38.033216337819866\n",
            "epoch  208  loss =  38.03116280592903\n",
            "epoch  209  loss =  38.02912868236753\n",
            "epoch  210  loss =  38.02711369729412\n",
            "epoch  211  loss =  38.02511758577328\n",
            "epoch  212  loss =  38.023140087665894\n",
            "epoch  213  loss =  38.02118094752284\n",
            "epoch  214  loss =  38.01923991448139\n",
            "epoch  215  loss =  38.0173167421641\n",
            "epoch  216  loss =  38.01541118858062\n",
            "epoch  217  loss =  38.01352301603179\n",
            "epoch  218  loss =  38.01165199101632\n",
            "epoch  219  loss =  38.00979788413979\n",
            "epoch  220  loss =  38.007960470026106\n",
            "epoch  221  loss =  38.00613952723097\n",
            "epoch  222  loss =  38.004334838157845\n",
            "epoch  223  loss =  38.00254618897574\n",
            "epoch  224  loss =  38.0007733695393\n",
            "epoch  225  loss =  37.99901617331071\n",
            "epoch  226  loss =  37.99727439728368\n",
            "epoch  227  loss =  37.99554784190931\n",
            "epoch  228  loss =  37.993836311023635\n",
            "epoch  229  loss =  37.99213961177719\n",
            "epoch  230  loss =  37.99045755456611\n",
            "epoch  231  loss =  37.988789952965064\n",
            "epoch  232  loss =  37.98713662366172\n",
            "epoch  233  loss =  37.98549738639279\n",
            "epoch  234  loss =  37.98387206388179\n",
            "epoch  235  loss =  37.98226048177806\n",
            "epoch  236  loss =  37.98066246859753\n",
            "epoch  237  loss =  37.979077855664585\n",
            "epoch  238  loss =  37.977506477055606\n",
            "epoch  239  loss =  37.97594816954364\n",
            "epoch  240  loss =  37.974402772544586\n",
            "epoch  241  loss =  37.9728701280644\n",
            "epoch  242  loss =  37.9713500806478\n",
            "epoch  243  loss =  37.96984247732804\n",
            "epoch  244  loss =  37.96834716757777\n",
            "epoch  245  loss =  37.96686400326125\n",
            "epoch  246  loss =  37.96539283858753\n",
            "epoch  247  loss =  37.96393353006477\n",
            "epoch  248  loss =  37.96248593645553\n",
            "epoch  249  loss =  37.96104991873319\n",
            "epoch  250  loss =  37.95962534003933\n",
            "epoch  251  loss =  37.95821206564207\n",
            "epoch  252  loss =  37.95680996289532\n",
            "epoch  253  loss =  37.955418901199025\n",
            "epoch  254  loss =  37.9540387519603\n",
            "epoch  255  loss =  37.95266938855538\n",
            "epoch  256  loss =  37.95131068629245\n",
            "epoch  257  loss =  37.94996252237531\n",
            "epoch  258  loss =  37.94862477586791\n",
            "epoch  259  loss =  37.94729732765953\n",
            "epoch  260  loss =  37.94598006043084\n",
            "epoch  261  loss =  37.94467285862073\n",
            "epoch  262  loss =  37.94337560839368\n",
            "epoch  263  loss =  37.942088197608165\n",
            "epoch  264  loss =  37.9408105157854\n",
            "epoch  265  loss =  37.93954245407904\n",
            "epoch  266  loss =  37.93828390524532\n",
            "epoch  267  loss =  37.937034763614065\n",
            "epoch  268  loss =  37.93579492506011\n",
            "epoch  269  loss =  37.93456428697544\n",
            "epoch  270  loss =  37.933342748241934\n",
            "epoch  271  loss =  37.932130209204686\n",
            "epoch  272  loss =  37.930926571645806\n",
            "epoch  273  loss =  37.92973173875886\n",
            "epoch  274  loss =  37.92854561512388\n",
            "epoch  275  loss =  37.927368106682835\n",
            "epoch  276  loss =  37.926199120715566\n",
            "epoch  277  loss =  37.92503856581635\n",
            "epoch  278  loss =  37.92388635187089\n",
            "epoch  279  loss =  37.92274239003376\n",
            "epoch  280  loss =  37.92160659270635\n",
            "epoch  281  loss =  37.92047887351521\n",
            "epoch  282  loss =  37.91935914729102\n",
            "epoch  283  loss =  37.918247330047635\n",
            "epoch  284  loss =  37.91714333896202\n",
            "epoch  285  loss =  37.916047092354184\n",
            "epoch  286  loss =  37.91495850966769\n",
            "epoch  287  loss =  37.913877511450686\n",
            "epoch  288  loss =  37.912804019337074\n",
            "epoch  289  loss =  37.91173795602819\n",
            "epoch  290  loss =  37.910679245274856\n",
            "epoch  291  loss =  37.90962781185983\n",
            "epoch  292  loss =  37.90858358158035\n",
            "epoch  293  loss =  37.90754648123148\n",
            "epoch  294  loss =  37.906516438589286\n",
            "epoch  295  loss =  37.90549338239475\n",
            "epoch  296  loss =  37.904477242337755\n",
            "epoch  297  loss =  37.90346794904146\n",
            "epoch  298  loss =  37.902465434047016\n",
            "epoch  299  loss =  37.901469629798505\n",
            "epoch  300  loss =  37.900480469628235\n",
            "epoch  301  loss =  37.8994978877423\n",
            "epoch  302  loss =  37.89852181920641\n",
            "epoch  303  loss =  37.89755219993205\n",
            "epoch  304  loss =  37.89658896666265\n",
            "epoch  305  loss =  37.89563205696056\n",
            "epoch  306  loss =  37.89468140919362\n",
            "epoch  307  loss =  37.893736962522496\n",
            "epoch  308  loss =  37.892798656887976\n",
            "epoch  309  loss =  37.891866432998626\n",
            "epoch  310  loss =  37.89094023231862\n",
            "epoch  311  loss =  37.89001999705587\n",
            "epoch  312  loss =  37.889105670150265\n",
            "epoch  313  loss =  37.88819719526225\n",
            "epoch  314  loss =  37.88729451676153\n",
            "epoch  315  loss =  37.886397579716075\n",
            "epoch  316  loss =  37.88550632988119\n",
            "epoch  317  loss =  37.884620713688925\n",
            "epoch  318  loss =  37.883740678237594\n",
            "epoch  319  loss =  37.88286617128157\n",
            "epoch  320  loss =  37.881997141221106\n",
            "epoch  321  loss =  37.88113353709258\n",
            "epoch  322  loss =  37.880275308558666\n",
            "epoch  323  loss =  37.879422405898914\n",
            "epoch  324  loss =  37.878574780000285\n",
            "epoch  325  loss =  37.877732382348\n",
            "epoch  326  loss =  37.87689516501658\n",
            "epoch  327  loss =  37.87606308066089\n",
            "epoch  328  loss =  37.87523608250746\n",
            "epoch  329  loss =  37.87441412434597\n",
            "epoch  330  loss =  37.87359716052077\n",
            "epoch  331  loss =  37.87278514592278\n",
            "epoch  332  loss =  37.87197803598122\n",
            "epoch  333  loss =  37.871175786655755\n",
            "epoch  334  loss =  37.87037835442864\n",
            "epoch  335  loss =  37.86958569629702\n",
            "epoch  336  loss =  37.86879776976545\n",
            "epoch  337  loss =  37.8680145328384\n",
            "epoch  338  loss =  37.867235944013004\n",
            "epoch  339  loss =  37.86646196227188\n",
            "epoch  340  loss =  37.86569254707611\n",
            "epoch  341  loss =  37.86492765835832\n",
            "epoch  342  loss =  37.864167256515906\n",
            "epoch  343  loss =  37.86341130240429\n",
            "epoch  344  loss =  37.86265975733039\n",
            "epoch  345  loss =  37.86191258304621\n",
            "epoch  346  loss =  37.8611697417424\n",
            "epoch  347  loss =  37.86043119604209\n",
            "epoch  348  loss =  37.85969690899474\n",
            "epoch  349  loss =  37.85896684407012\n",
            "epoch  350  loss =  37.85824096515237\n",
            "epoch  351  loss =  37.857519236534145\n",
            "epoch  352  loss =  37.856801622910936\n",
            "epoch  353  loss =  37.85608808937544\n",
            "epoch  354  loss =  37.85537860141195\n",
            "epoch  355  loss =  37.85467312489101\n",
            "epoch  356  loss =  37.85397162606392\n",
            "epoch  357  loss =  37.85327407155764\n",
            "epoch  358  loss =  37.852580428369414\n",
            "epoch  359  loss =  37.85189066386186\n",
            "epoch  360  loss =  37.85120474575777\n",
            "epoch  361  loss =  37.85052264213539\n",
            "epoch  362  loss =  37.84984432142336\n",
            "epoch  363  loss =  37.84916975239607\n",
            "epoch  364  loss =  37.84849890416893\n",
            "epoch  365  loss =  37.84783174619372\n",
            "epoch  366  loss =  37.847168248254114\n",
            "epoch  367  loss =  37.84650838046116\n",
            "epoch  368  loss =  37.84585211324882\n",
            "epoch  369  loss =  37.84519941736983\n",
            "epoch  370  loss =  37.84455026389118\n",
            "epoch  371  loss =  37.84390462419019\n",
            "epoch  372  loss =  37.84326246995014\n",
            "epoch  373  loss =  37.8426237731564\n",
            "epoch  374  loss =  37.841988506092285\n",
            "epoch  375  loss =  37.84135664133525\n",
            "epoch  376  loss =  37.840728151752955\n",
            "epoch  377  loss =  37.8401030104994\n",
            "epoch  378  loss =  37.839481191011274\n",
            "epoch  379  loss =  37.83886266700423\n",
            "epoch  380  loss =  37.83824741246922\n",
            "epoch  381  loss =  37.83763540166892\n",
            "epoch  382  loss =  37.83702660913425\n",
            "epoch  383  loss =  37.83642100966086\n",
            "epoch  384  loss =  37.83581857830576\n",
            "epoch  385  loss =  37.83521929038387\n",
            "epoch  386  loss =  37.83462312146478\n",
            "epoch  387  loss =  37.83403004736955\n",
            "epoch  388  loss =  37.833440044167304\n",
            "epoch  389  loss =  37.83285308817227\n",
            "epoch  390  loss =  37.83226915594059\n",
            "epoch  391  loss =  37.83168822426717\n",
            "epoch  392  loss =  37.8311102701828\n",
            "epoch  393  loss =  37.8305352709511\n",
            "epoch  394  loss =  37.82996320406559\n",
            "epoch  395  loss =  37.829394047246815\n",
            "epoch  396  loss =  37.828827778439546\n",
            "epoch  397  loss =  37.82826437580987\n",
            "epoch  398  loss =  37.82770381774254\n",
            "epoch  399  loss =  37.827146082838205\n",
            "epoch  400  loss =  37.826591149910755\n",
            "epoch  401  loss =  37.82603899798463\n",
            "epoch  402  loss =  37.825489606292315\n",
            "epoch  403  loss =  37.82494295427165\n",
            "epoch  404  loss =  37.82439902156347\n",
            "epoch  405  loss =  37.823857788008965\n",
            "epoch  406  loss =  37.823319233647275\n",
            "epoch  407  loss =  37.82278333871318\n",
            "epoch  408  loss =  37.822250083634565\n",
            "epoch  409  loss =  37.82171944903013\n",
            "epoch  410  loss =  37.82119141570716\n",
            "epoch  411  loss =  37.820665964659135\n",
            "epoch  412  loss =  37.82014307706352\n",
            "epoch  413  loss =  37.81962273427964\n",
            "epoch  414  loss =  37.8191049178463\n",
            "epoch  415  loss =  37.81858960947987\n",
            "epoch  416  loss =  37.818076791071974\n",
            "epoch  417  loss =  37.81756644468754\n",
            "epoch  418  loss =  37.817058552562656\n",
            "epoch  419  loss =  37.81655309710256\n",
            "epoch  420  loss =  37.816050060879654\n",
            "epoch  421  loss =  37.81554942663151\n",
            "epoch  422  loss =  37.815051177258994\n",
            "epoch  423  loss =  37.81455529582419\n",
            "epoch  424  loss =  37.8140617655487\n",
            "epoch  425  loss =  37.813570569811645\n",
            "epoch  426  loss =  37.813081692147875\n",
            "epoch  427  loss =  37.81259511624617\n",
            "epoch  428  loss =  37.81211082594742\n",
            "epoch  429  loss =  37.811628805242876\n",
            "epoch  430  loss =  37.811149038272426\n",
            "epoch  431  loss =  37.810671509322866\n",
            "epoch  432  loss =  37.81019620282616\n",
            "epoch  433  loss =  37.809723103357896\n",
            "epoch  434  loss =  37.809252195635516\n",
            "epoch  435  loss =  37.808783464516765\n",
            "epoch  436  loss =  37.808316894998065\n",
            "epoch  437  loss =  37.80785247221293\n",
            "epoch  438  loss =  37.80739018143047\n",
            "epoch  439  loss =  37.806930008053705\n",
            "epoch  440  loss =  37.80647193761827\n",
            "epoch  441  loss =  37.80601595579069\n",
            "epoch  442  loss =  37.80556204836711\n",
            "epoch  443  loss =  37.80511020127166\n",
            "epoch  444  loss =  37.80466040055518\n",
            "epoch  445  loss =  37.80421263239368\n",
            "epoch  446  loss =  37.803766883087015\n",
            "epoch  447  loss =  37.80332313905744\n",
            "epoch  448  loss =  37.80288138684837\n",
            "epoch  449  loss =  37.80244161312293\n",
            "epoch  450  loss =  37.8020038046626\n",
            "epoch  451  loss =  37.801567948366056\n",
            "epoch  452  loss =  37.80113403124771\n",
            "epoch  453  loss =  37.80070204043662\n",
            "epoch  454  loss =  37.800271963175035\n",
            "epoch  455  loss =  37.799843786817334\n",
            "epoch  456  loss =  37.799417498828646\n",
            "epoch  457  loss =  37.798993086783796\n",
            "epoch  458  loss =  37.79857053836597\n",
            "epoch  459  loss =  37.79814984136564\n",
            "epoch  460  loss =  37.79773098367937\n",
            "epoch  461  loss =  37.797313953308645\n",
            "epoch  462  loss =  37.79689873835878\n",
            "epoch  463  loss =  37.79648532703777\n",
            "epoch  464  loss =  37.79607370765524\n",
            "epoch  465  loss =  37.79566386862125\n",
            "epoch  466  loss =  37.79525579844534\n",
            "epoch  467  loss =  37.794849485735426\n",
            "epoch  468  loss =  37.79444491919673\n",
            "epoch  469  loss =  37.79404208763073\n",
            "epoch  470  loss =  37.793640979934224\n",
            "epoch  471  loss =  37.79324158509823\n",
            "epoch  472  loss =  37.79284389220706\n",
            "epoch  473  loss =  37.79244789043728\n",
            "epoch  474  loss =  37.79205356905679\n",
            "epoch  475  loss =  37.79166091742382\n",
            "epoch  476  loss =  37.79126992498601\n",
            "epoch  477  loss =  37.79088058127952\n",
            "epoch  478  loss =  37.79049287592797\n",
            "epoch  479  loss =  37.790106798641695\n",
            "epoch  480  loss =  37.78972233921673\n",
            "epoch  481  loss =  37.789339487534\n",
            "epoch  482  loss =  37.788958233558354\n",
            "epoch  483  loss =  37.78857856733778\n",
            "epoch  484  loss =  37.78820047900249\n",
            "epoch  485  loss =  37.78782395876413\n",
            "epoch  486  loss =  37.787448996914904\n",
            "epoch  487  loss =  37.78707558382673\n",
            "epoch  488  loss =  37.78670370995051\n",
            "epoch  489  loss =  37.78633336581522\n",
            "epoch  490  loss =  37.78596454202721\n",
            "epoch  491  loss =  37.785597229269314\n",
            "epoch  492  loss =  37.785231418300214\n",
            "epoch  493  loss =  37.78486709995353\n",
            "epoch  494  loss =  37.784504265137144\n",
            "epoch  495  loss =  37.78414290483244\n",
            "epoch  496  loss =  37.78378301009356\n",
            "epoch  497  loss =  37.78342457204663\n",
            "epoch  498  loss =  37.78306758188917\n",
            "epoch  499  loss =  37.78271203088923\n",
            "epoch  500  loss =  37.78235791038472\n",
            "epoch  501  loss =  37.78200521178283\n",
            "epoch  502  loss =  37.781653926559166\n",
            "epoch  503  loss =  37.7813040462572\n",
            "epoch  504  loss =  37.780955562487584\n",
            "epoch  505  loss =  37.78060846692739\n",
            "epoch  506  loss =  37.78026275131959\n",
            "epoch  507  loss =  37.779918407472316\n",
            "epoch  508  loss =  37.779575427258216\n",
            "epoch  509  loss =  37.77923380261391\n",
            "epoch  510  loss =  37.778893525539246\n",
            "epoch  511  loss =  37.77855458809677\n",
            "epoch  512  loss =  37.77821698241114\n",
            "epoch  513  loss =  37.77788070066835\n",
            "epoch  514  loss =  37.77754573511534\n",
            "epoch  515  loss =  37.7772120780593\n",
            "epoch  516  loss =  37.776879721867076\n",
            "epoch  517  loss =  37.77654865896469\n",
            "epoch  518  loss =  37.7762188818366\n",
            "epoch  519  loss =  37.77589038302533\n",
            "epoch  520  loss =  37.7755631551308\n",
            "epoch  521  loss =  37.77523719080977\n",
            "epoch  522  loss =  37.77491248277535\n",
            "epoch  523  loss =  37.77458902379642\n",
            "epoch  524  loss =  37.77426680669716\n",
            "epoch  525  loss =  37.77394582435643\n",
            "epoch  526  loss =  37.773626069707305\n",
            "epoch  527  loss =  37.77330753573661\n",
            "epoch  528  loss =  37.772990215484285\n",
            "epoch  529  loss =  37.77267410204299\n",
            "epoch  530  loss =  37.77235918855761\n",
            "epoch  531  loss =  37.772045468224675\n",
            "epoch  532  loss =  37.771732934291954\n",
            "epoch  533  loss =  37.77142158005795\n",
            "epoch  534  loss =  37.77111139887143\n",
            "epoch  535  loss =  37.77080238413096\n",
            "epoch  536  loss =  37.77049452928439\n",
            "epoch  537  loss =  37.77018782782849\n",
            "epoch  538  loss =  37.76988227330843\n",
            "epoch  539  loss =  37.76957785931732\n",
            "epoch  540  loss =  37.76927457949582\n",
            "epoch  541  loss =  37.76897242753167\n",
            "epoch  542  loss =  37.768671397159224\n",
            "epoch  543  loss =  37.76837148215914\n",
            "epoch  544  loss =  37.768072676357754\n",
            "epoch  545  loss =  37.767774973626885\n",
            "epoch  546  loss =  37.76747836788326\n",
            "epoch  547  loss =  37.76718285308816\n",
            "epoch  548  loss =  37.766888423246996\n",
            "epoch  549  loss =  37.76659507240896\n",
            "epoch  550  loss =  37.76630279466659\n",
            "epoch  551  loss =  37.76601158415532\n",
            "epoch  552  loss =  37.7657214350532\n",
            "epoch  553  loss =  37.76543234158041\n",
            "epoch  554  loss =  37.765144297998994\n",
            "epoch  555  loss =  37.76485729861236\n",
            "epoch  556  loss =  37.76457133776498\n",
            "epoch  557  loss =  37.764286409842036\n",
            "epoch  558  loss =  37.76400250926896\n",
            "epoch  559  loss =  37.76371963051125\n",
            "epoch  560  loss =  37.763437768073864\n",
            "epoch  561  loss =  37.76315691650112\n",
            "epoch  562  loss =  37.76287707037619\n",
            "epoch  563  loss =  37.762598224320776\n",
            "epoch  564  loss =  37.76232037299484\n",
            "epoch  565  loss =  37.7620435110962\n",
            "epoch  566  loss =  37.76176763336019\n",
            "epoch  567  loss =  37.761492734559354\n",
            "epoch  568  loss =  37.76121880950314\n",
            "epoch  569  loss =  37.760945853037555\n",
            "epoch  570  loss =  37.76067386004479\n",
            "epoch  571  loss =  37.76040282544301\n",
            "epoch  572  loss =  37.76013274418597\n",
            "epoch  573  loss =  37.759863611262695\n",
            "epoch  574  loss =  37.75959542169722\n",
            "epoch  575  loss =  37.75932817054825\n",
            "epoch  576  loss =  37.759061852908886\n",
            "epoch  577  loss =  37.7587964639063\n",
            "epoch  578  loss =  37.75853199870143\n",
            "epoch  579  loss =  37.75826845248877\n",
            "epoch  580  loss =  37.75800582049595\n",
            "epoch  581  loss =  37.7577440979836\n",
            "epoch  582  loss =  37.75748328024491\n",
            "epoch  583  loss =  37.757223362605465\n",
            "epoch  584  loss =  37.756964340422925\n",
            "epoch  585  loss =  37.75670620908681\n",
            "epoch  586  loss =  37.75644896401809\n",
            "epoch  587  loss =  37.75619260066905\n",
            "epoch  588  loss =  37.75593711452297\n",
            "epoch  589  loss =  37.75568250109389\n",
            "epoch  590  loss =  37.75542875592629\n",
            "epoch  591  loss =  37.755175874594855\n",
            "epoch  592  loss =  37.754923852704316\n",
            "epoch  593  loss =  37.75467268588904\n",
            "epoch  594  loss =  37.75442236981287\n",
            "epoch  595  loss =  37.7541729001689\n",
            "epoch  596  loss =  37.75392427267917\n",
            "epoch  597  loss =  37.753676483094424\n",
            "epoch  598  loss =  37.75342952719391\n",
            "epoch  599  loss =  37.753183400785154\n",
            "epoch  600  loss =  37.75293809970364\n",
            "epoch  601  loss =  37.75269361981265\n",
            "epoch  602  loss =  37.75244995700306\n",
            "epoch  603  loss =  37.75220710719301\n",
            "epoch  604  loss =  37.75196506632775\n",
            "epoch  605  loss =  37.75172383037939\n",
            "epoch  606  loss =  37.75148339534673\n",
            "epoch  607  loss =  37.75124375725496\n",
            "epoch  608  loss =  37.751004912155466\n",
            "epoch  609  loss =  37.75076685612571\n",
            "epoch  610  loss =  37.750529585268815\n",
            "epoch  611  loss =  37.75029309571357\n",
            "epoch  612  loss =  37.750057383614106\n",
            "epoch  613  loss =  37.749822445149704\n",
            "epoch  614  loss =  37.74958827652456\n",
            "epoch  615  loss =  37.749354873967704\n",
            "epoch  616  loss =  37.74912223373259\n",
            "epoch  617  loss =  37.74889035209715\n",
            "epoch  618  loss =  37.74865922536336\n",
            "epoch  619  loss =  37.74842884985722\n",
            "epoch  620  loss =  37.748199221928445\n",
            "epoch  621  loss =  37.74797033795036\n",
            "epoch  622  loss =  37.74774219431965\n",
            "epoch  623  loss =  37.747514787456176\n",
            "epoch  624  loss =  37.74728811380287\n",
            "epoch  625  loss =  37.74706216982537\n",
            "epoch  626  loss =  37.74683695201209\n",
            "epoch  627  loss =  37.74661245687382\n",
            "epoch  628  loss =  37.74638868094364\n",
            "epoch  629  loss =  37.74616562077674\n",
            "epoch  630  loss =  37.74594327295026\n",
            "epoch  631  loss =  37.74572163406306\n",
            "epoch  632  loss =  37.74550070073558\n",
            "epoch  633  loss =  37.74528046960973\n",
            "epoch  634  loss =  37.74506093734857\n",
            "epoch  635  loss =  37.74484210063634\n",
            "epoch  636  loss =  37.744623956178096\n",
            "epoch  637  loss =  37.744406500699675\n",
            "epoch  638  loss =  37.744189730947475\n",
            "epoch  639  loss =  37.74397364368839\n",
            "epoch  640  loss =  37.74375823570948\n",
            "epoch  641  loss =  37.74354350381795\n",
            "epoch  642  loss =  37.74332944484097\n",
            "epoch  643  loss =  37.74311605562544\n",
            "epoch  644  loss =  37.74290333303795\n",
            "epoch  645  loss =  37.742691273964574\n",
            "epoch  646  loss =  37.742479875310686\n",
            "epoch  647  loss =  37.742269134000885\n",
            "epoch  648  loss =  37.742059046978774\n",
            "epoch  649  loss =  37.741849611206845\n",
            "epoch  650  loss =  37.74164082366635\n",
            "epoch  651  loss =  37.741432681357125\n",
            "epoch  652  loss =  37.74122518129747\n",
            "epoch  653  loss =  37.741018320524006\n",
            "epoch  654  loss =  37.740812096091524\n",
            "epoch  655  loss =  37.74060650507284\n",
            "epoch  656  loss =  37.74040154455869\n",
            "epoch  657  loss =  37.740197211657545\n",
            "epoch  658  loss =  37.73999350349551\n",
            "epoch  659  loss =  37.739790417216234\n",
            "epoch  660  loss =  37.73958794998062\n",
            "epoch  661  loss =  37.73938609896691\n",
            "epoch  662  loss =  37.739184861370376\n",
            "epoch  663  loss =  37.73898423440328\n",
            "epoch  664  loss =  37.73878421529472\n",
            "epoch  665  loss =  37.73858480129053\n",
            "epoch  666  loss =  37.738385989653096\n",
            "epoch  667  loss =  37.7381877776613\n",
            "epoch  668  loss =  37.73799016261034\n",
            "epoch  669  loss =  37.73779314181165\n",
            "epoch  670  loss =  37.73759671259277\n",
            "epoch  671  loss =  37.73740087229718\n",
            "epoch  672  loss =  37.73720561828426\n",
            "epoch  673  loss =  37.73701094792909\n",
            "epoch  674  loss =  37.73681685862243\n",
            "epoch  675  loss =  37.7366233477705\n",
            "epoch  676  loss =  37.7364304127949\n",
            "epoch  677  loss =  37.73623805113256\n",
            "epoch  678  loss =  37.73604626023554\n",
            "epoch  679  loss =  37.73585503757098\n",
            "epoch  680  loss =  37.735664380620904\n",
            "epoch  681  loss =  37.73547428688228\n",
            "epoch  682  loss =  37.735284753866665\n",
            "epoch  683  loss =  37.73509577910035\n",
            "epoch  684  loss =  37.734907360124055\n",
            "epoch  685  loss =  37.73471949449295\n",
            "epoch  686  loss =  37.734532179776465\n",
            "epoch  687  loss =  37.734345413558266\n",
            "epoch  688  loss =  37.73415919343609\n",
            "epoch  689  loss =  37.733973517021646\n",
            "epoch  690  loss =  37.733788381940556\n",
            "epoch  691  loss =  37.73360378583221\n",
            "epoch  692  loss =  37.73341972634971\n",
            "epoch  693  loss =  37.733236201159706\n",
            "epoch  694  loss =  37.73305320794237\n",
            "epoch  695  loss =  37.73287074439126\n",
            "epoch  696  loss =  37.7326888082132\n",
            "epoch  697  loss =  37.732507397128245\n",
            "epoch  698  loss =  37.73232650886956\n",
            "epoch  699  loss =  37.732146141183314\n",
            "epoch  700  loss =  37.731966291828584\n",
            "epoch  701  loss =  37.73178695857727\n",
            "epoch  702  loss =  37.731608139214025\n",
            "epoch  703  loss =  37.731429831536154\n",
            "epoch  704  loss =  37.73125203335347\n",
            "epoch  705  loss =  37.73107474248829\n",
            "epoch  706  loss =  37.730897956775316\n",
            "epoch  707  loss =  37.7307216740615\n",
            "epoch  708  loss =  37.73054589220601\n",
            "epoch  709  loss =  37.73037060908016\n",
            "epoch  710  loss =  37.73019582256725\n",
            "epoch  711  loss =  37.73002153056256\n",
            "epoch  712  loss =  37.72984773097319\n",
            "epoch  713  loss =  37.72967442171808\n",
            "epoch  714  loss =  37.72950160072781\n",
            "epoch  715  loss =  37.7293292659446\n",
            "epoch  716  loss =  37.72915741532217\n",
            "epoch  717  loss =  37.728986046825746\n",
            "epoch  718  loss =  37.728815158431885\n",
            "epoch  719  loss =  37.72864474812846\n",
            "epoch  720  loss =  37.728474813914524\n",
            "epoch  721  loss =  37.728305353800316\n",
            "epoch  722  loss =  37.72813636580707\n",
            "epoch  723  loss =  37.727967847967065\n",
            "epoch  724  loss =  37.727799798323446\n",
            "epoch  725  loss =  37.7276322149302\n",
            "epoch  726  loss =  37.72746509585209\n",
            "epoch  727  loss =  37.727298439164514\n",
            "epoch  728  loss =  37.72713224295354\n",
            "epoch  729  loss =  37.72696650531571\n",
            "epoch  730  loss =  37.72680122435805\n",
            "epoch  731  loss =  37.72663639819801\n",
            "epoch  732  loss =  37.72647202496333\n",
            "epoch  733  loss =  37.726308102792004\n",
            "epoch  734  loss =  37.726144629832206\n",
            "epoch  735  loss =  37.72598160424219\n",
            "epoch  736  loss =  37.72581902419036\n",
            "epoch  737  loss =  37.72565688785494\n",
            "epoch  738  loss =  37.72549519342416\n",
            "epoch  739  loss =  37.72533393909607\n",
            "epoch  740  loss =  37.7251731230785\n",
            "epoch  741  loss =  37.72501274358895\n",
            "epoch  742  loss =  37.724852798854585\n",
            "epoch  743  loss =  37.72469328711213\n",
            "epoch  744  loss =  37.72453420660788\n",
            "epoch  745  loss =  37.72437555559748\n",
            "epoch  746  loss =  37.72421733234605\n",
            "epoch  747  loss =  37.72405953512797\n",
            "epoch  748  loss =  37.723902162226885\n",
            "epoch  749  loss =  37.723745211935665\n",
            "epoch  750  loss =  37.7235886825563\n",
            "epoch  751  loss =  37.72343257239984\n",
            "epoch  752  loss =  37.723276879786354\n",
            "epoch  753  loss =  37.723121603044916\n",
            "epoch  754  loss =  37.72296674051338\n",
            "epoch  755  loss =  37.722812290538556\n",
            "epoch  756  loss =  37.722658251475956\n",
            "epoch  757  loss =  37.72250462168984\n",
            "epoch  758  loss =  37.722351399553105\n",
            "epoch  759  loss =  37.72219858344728\n",
            "epoch  760  loss =  37.72204617176243\n",
            "epoch  761  loss =  37.7218941628971\n",
            "epoch  762  loss =  37.72174255525826\n",
            "epoch  763  loss =  37.721591347261324\n",
            "epoch  764  loss =  37.721440537329926\n",
            "epoch  765  loss =  37.721290123896054\n",
            "epoch  766  loss =  37.721140105399876\n",
            "epoch  767  loss =  37.72099048028973\n",
            "epoch  768  loss =  37.72084124702207\n",
            "epoch  769  loss =  37.7206924040614\n",
            "epoch  770  loss =  37.72054394988021\n",
            "epoch  771  loss =  37.72039588295896\n",
            "epoch  772  loss =  37.720248201786\n",
            "epoch  773  loss =  37.72010090485754\n",
            "epoch  774  loss =  37.7199539906776\n",
            "epoch  775  loss =  37.719807457757895\n",
            "epoch  776  loss =  37.7196613046179\n",
            "epoch  777  loss =  37.71951552978468\n",
            "epoch  778  loss =  37.71937013179295\n",
            "epoch  779  loss =  37.71922510918496\n",
            "epoch  780  loss =  37.719080460510426\n",
            "epoch  781  loss =  37.71893618432655\n",
            "epoch  782  loss =  37.71879227919795\n",
            "epoch  783  loss =  37.718648743696576\n",
            "epoch  784  loss =  37.71850557640166\n",
            "epoch  785  loss =  37.71836277589979\n",
            "epoch  786  loss =  37.718220340784704\n",
            "epoch  787  loss =  37.7180782696573\n",
            "epoch  788  loss =  37.717936561125626\n",
            "epoch  789  loss =  37.71779521380484\n",
            "epoch  790  loss =  37.717654226317094\n",
            "epoch  791  loss =  37.71751359729153\n",
            "epoch  792  loss =  37.71737332536426\n",
            "epoch  793  loss =  37.71723340917828\n",
            "epoch  794  loss =  37.71709384738349\n",
            "epoch  795  loss =  37.71695463863654\n",
            "epoch  796  loss =  37.71681578160088\n",
            "epoch  797  loss =  37.71667727494671\n",
            "epoch  798  loss =  37.716539117350926\n",
            "epoch  799  loss =  37.71640130749701\n",
            "epoch  800  loss =  37.716263844075115\n",
            "epoch  801  loss =  37.716126725781905\n",
            "epoch  802  loss =  37.71598995132064\n",
            "epoch  803  loss =  37.715853519400966\n",
            "epoch  804  loss =  37.71571742873901\n",
            "epoch  805  loss =  37.71558167805737\n",
            "epoch  806  loss =  37.71544626608491\n",
            "epoch  807  loss =  37.71531119155685\n",
            "epoch  808  loss =  37.715176453214696\n",
            "epoch  809  loss =  37.71504204980617\n",
            "epoch  810  loss =  37.71490798008531\n",
            "epoch  811  loss =  37.714774242812126\n",
            "epoch  812  loss =  37.714640836752935\n",
            "epoch  813  loss =  37.71450776068006\n",
            "epoch  814  loss =  37.714375013371885\n",
            "epoch  815  loss =  37.71424259361284\n",
            "epoch  816  loss =  37.71411050019328\n",
            "epoch  817  loss =  37.71397873190959\n",
            "epoch  818  loss =  37.71384728756396\n",
            "epoch  819  loss =  37.71371616596452\n",
            "epoch  820  loss =  37.71358536592522\n",
            "epoch  821  loss =  37.71345488626578\n",
            "epoch  822  loss =  37.71332472581171\n",
            "epoch  823  loss =  37.71319488339426\n",
            "epoch  824  loss =  37.71306535785034\n",
            "epoch  825  loss =  37.71293614802254\n",
            "epoch  826  loss =  37.712807252759085\n",
            "epoch  827  loss =  37.71267867091373\n",
            "epoch  828  loss =  37.71255040134588\n",
            "epoch  829  loss =  37.71242244292038\n",
            "epoch  830  loss =  37.71229479450761\n",
            "epoch  831  loss =  37.71216745498341\n",
            "epoch  832  loss =  37.71204042322901\n",
            "epoch  833  loss =  37.71191369813107\n",
            "epoch  834  loss =  37.71178727858157\n",
            "epoch  835  loss =  37.71166116347782\n",
            "epoch  836  loss =  37.711535351722475\n",
            "epoch  837  loss =  37.71140984222339\n",
            "epoch  838  loss =  37.71128463389369\n",
            "epoch  839  loss =  37.71115972565168\n",
            "epoch  840  loss =  37.71103511642084\n",
            "epoch  841  loss =  37.710910805129814\n",
            "epoch  842  loss =  37.710786790712305\n",
            "epoch  843  loss =  37.71066307210711\n",
            "epoch  844  loss =  37.710539648258084\n",
            "epoch  845  loss =  37.71041651811412\n",
            "epoch  846  loss =  37.71029368062906\n",
            "epoch  847  loss =  37.71017113476172\n",
            "epoch  848  loss =  37.710048879475835\n",
            "epoch  849  loss =  37.70992691374003\n",
            "epoch  850  loss =  37.709805236527856\n",
            "epoch  851  loss =  37.70968384681765\n",
            "epoch  852  loss =  37.70956274359258\n",
            "epoch  853  loss =  37.70944192584058\n",
            "epoch  854  loss =  37.70932139255436\n",
            "epoch  855  loss =  37.709201142731395\n",
            "epoch  856  loss =  37.709081175373804\n",
            "epoch  857  loss =  37.7089614894884\n",
            "epoch  858  loss =  37.708842084086626\n",
            "epoch  859  loss =  37.70872295818458\n",
            "epoch  860  loss =  37.70860411080295\n",
            "epoch  861  loss =  37.708485540966926\n",
            "epoch  862  loss =  37.70836724770634\n",
            "epoch  863  loss =  37.70824923005545\n",
            "epoch  864  loss =  37.70813148705303\n",
            "epoch  865  loss =  37.70801401774233\n",
            "epoch  866  loss =  37.70789682117102\n",
            "epoch  867  loss =  37.70777989639118\n",
            "epoch  868  loss =  37.707663242459276\n",
            "epoch  869  loss =  37.70754685843614\n",
            "epoch  870  loss =  37.7074307433869\n",
            "epoch  871  loss =  37.70731489638106\n",
            "epoch  872  loss =  37.707199316492336\n",
            "epoch  873  loss =  37.707084002798766\n",
            "epoch  874  loss =  37.70696895438256\n",
            "epoch  875  loss =  37.7068541703302\n",
            "epoch  876  loss =  37.7067396497323\n",
            "epoch  877  loss =  37.70662539168369\n",
            "epoch  878  loss =  37.70651139528329\n",
            "epoch  879  loss =  37.706397659634156\n",
            "epoch  880  loss =  37.70628418384343\n",
            "epoch  881  loss =  37.706170967022345\n",
            "epoch  882  loss =  37.70605800828613\n",
            "epoch  883  loss =  37.7059453067541\n",
            "epoch  884  loss =  37.705832861549524\n",
            "epoch  885  loss =  37.70572067179966\n",
            "epoch  886  loss =  37.70560873663572\n",
            "epoch  887  loss =  37.705497055192836\n",
            "epoch  888  loss =  37.705385626610095\n",
            "epoch  889  loss =  37.705274450030394\n",
            "epoch  890  loss =  37.705163524600565\n",
            "epoch  891  loss =  37.70505284947125\n",
            "epoch  892  loss =  37.704942423796936\n",
            "epoch  893  loss =  37.704832246735876\n",
            "epoch  894  loss =  37.70472231745013\n",
            "epoch  895  loss =  37.70461263510552\n",
            "epoch  896  loss =  37.70450319887159\n",
            "epoch  897  loss =  37.704394007921586\n",
            "epoch  898  loss =  37.70428506143251\n",
            "epoch  899  loss =  37.70417635858496\n",
            "epoch  900  loss =  37.70406789856324\n",
            "epoch  901  loss =  37.70395968055528\n",
            "epoch  902  loss =  37.70385170375261\n",
            "epoch  903  loss =  37.70374396735037\n",
            "epoch  904  loss =  37.70363647054726\n",
            "epoch  905  loss =  37.70352921254556\n",
            "epoch  906  loss =  37.70342219255103\n",
            "epoch  907  loss =  37.70331540977301\n",
            "epoch  908  loss =  37.70320886342431\n",
            "epoch  909  loss =  37.70310255272118\n",
            "epoch  910  loss =  37.7029964768834\n",
            "epoch  911  loss =  37.7028906351341\n",
            "epoch  912  loss =  37.702785026699935\n",
            "epoch  913  loss =  37.702679650810865\n",
            "epoch  914  loss =  37.70257450670029\n",
            "epoch  915  loss =  37.70246959360495\n",
            "epoch  916  loss =  37.702364910764906\n",
            "epoch  917  loss =  37.7022604574236\n",
            "epoch  918  loss =  37.70215623282776\n",
            "epoch  919  loss =  37.702052236227374\n",
            "epoch  920  loss =  37.70194846687575\n",
            "epoch  921  loss =  37.70184492402943\n",
            "epoch  922  loss =  37.70174160694818\n",
            "epoch  923  loss =  37.70163851489498\n",
            "epoch  924  loss =  37.70153564713607\n",
            "epoch  925  loss =  37.7014330029408\n",
            "epoch  926  loss =  37.70133058158175\n",
            "epoch  927  loss =  37.70122838233458\n",
            "epoch  928  loss =  37.701126404478174\n",
            "epoch  929  loss =  37.70102464729445\n",
            "epoch  930  loss =  37.700923110068466\n",
            "epoch  931  loss =  37.70082179208834\n",
            "epoch  932  loss =  37.70072069264528\n",
            "epoch  933  loss =  37.700619811033576\n",
            "epoch  934  loss =  37.70051914655044\n",
            "epoch  935  loss =  37.70041869849621\n",
            "epoch  936  loss =  37.70031846617418\n",
            "epoch  937  loss =  37.700218448890624\n",
            "epoch  938  loss =  37.70011864595483\n",
            "epoch  939  loss =  37.70001905667897\n",
            "epoch  940  loss =  37.69991968037818\n",
            "epoch  941  loss =  37.69982051637056\n",
            "epoch  942  loss =  37.69972156397705\n",
            "epoch  943  loss =  37.69962282252154\n",
            "epoch  944  loss =  37.69952429133074\n",
            "epoch  945  loss =  37.69942596973426\n",
            "epoch  946  loss =  37.699327857064546\n",
            "epoch  947  loss =  37.69922995265687\n",
            "epoch  948  loss =  37.699132255849314\n",
            "epoch  949  loss =  37.69903476598276\n",
            "epoch  950  loss =  37.69893748240088\n",
            "epoch  951  loss =  37.698840404450124\n",
            "epoch  952  loss =  37.6987435314797\n",
            "epoch  953  loss =  37.69864686284153\n",
            "epoch  954  loss =  37.69855039789027\n",
            "epoch  955  loss =  37.69845413598334\n",
            "epoch  956  loss =  37.69835807648078\n",
            "epoch  957  loss =  37.69826221874534\n",
            "epoch  958  loss =  37.698166562142504\n",
            "epoch  959  loss =  37.69807110604031\n",
            "epoch  960  loss =  37.697975849809524\n",
            "epoch  961  loss =  37.69788079282349\n",
            "epoch  962  loss =  37.69778593445819\n",
            "epoch  963  loss =  37.69769127409218\n",
            "epoch  964  loss =  37.69759681110665\n",
            "epoch  965  loss =  37.697502544885324\n",
            "epoch  966  loss =  37.69740847481449\n",
            "epoch  967  loss =  37.69731460028302\n",
            "epoch  968  loss =  37.69722092068228\n",
            "epoch  969  loss =  37.69712743540616\n",
            "epoch  970  loss =  37.69703414385109\n",
            "epoch  971  loss =  37.69694104541596\n",
            "epoch  972  loss =  37.696848139502194\n",
            "epoch  973  loss =  37.6967554255136\n",
            "epoch  974  loss =  37.69666290285651\n",
            "epoch  975  loss =  37.6965705709397\n",
            "epoch  976  loss =  37.696478429174356\n",
            "epoch  977  loss =  37.69638647697408\n",
            "epoch  978  loss =  37.6962947137549\n",
            "epoch  979  loss =  37.69620313893521\n",
            "epoch  980  loss =  37.696111751935824\n",
            "epoch  981  loss =  37.6960205521799\n",
            "epoch  982  loss =  37.69592953909295\n",
            "epoch  983  loss =  37.69583871210287\n",
            "epoch  984  loss =  37.69574807063983\n",
            "epoch  985  loss =  37.69565761413636\n",
            "epoch  986  loss =  37.695567342027324\n",
            "epoch  987  loss =  37.695477253749836\n",
            "epoch  988  loss =  37.69538734874331\n",
            "epoch  989  loss =  37.69529762644944\n",
            "epoch  990  loss =  37.69520808631222\n",
            "epoch  991  loss =  37.695118727777825\n",
            "epoch  992  loss =  37.69502955029472\n",
            "epoch  993  loss =  37.69494055331359\n",
            "epoch  994  loss =  37.69485173628735\n",
            "epoch  995  loss =  37.69476309867108\n",
            "epoch  996  loss =  37.69467463992211\n",
            "epoch  997  loss =  37.69458635949993\n",
            "epoch  998  loss =  37.69449825686619\n",
            "epoch  999  loss =  37.69441033148472\n",
            "['revolves', 'the', 'around']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añadiendo subsampling y negative sampling."
      ],
      "metadata": {
        "id": "IFhcfJeQuFn0"
      },
      "id": "IFhcfJeQuFn0"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def prepare_data_for_training(sentences, w2v, subsample_threshold=1e-5, negative_sampling_table_size=1e6, negative_sampling_power=0.75, negative_samples=5):\n",
        "    data = {}\n",
        "    word_counts = {}\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "            total_words += 1\n",
        "\n",
        "    subsampled_data = {}\n",
        "    for word, freq in data.items():\n",
        "        prob = 1 - np.sqrt(subsample_threshold / (freq / total_words))\n",
        "        if random.random() < prob:\n",
        "            continue\n",
        "        subsampled_data[word] = freq\n",
        "\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    subsampled_vocab = {word: i for i, (word, _) in enumerate(sorted(subsampled_data.items()))}\n",
        "\n",
        "    sampling_table = []\n",
        "    sampling_table_weights = []\n",
        "    for word, freq in subsampled_data.items():\n",
        "        sampling_table_weights.append(freq ** negative_sampling_power)\n",
        "    total_weight = sum(sampling_table_weights)\n",
        "    for i, weight in enumerate(sampling_table_weights):\n",
        "        prob = weight / total_weight\n",
        "        sampling_table.extend([i] * int(prob * negative_sampling_table_size))\n",
        "    sampling_table = np.array(sampling_table)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for i, center_word in enumerate(sentence):\n",
        "            if center_word not in subsampled_vocab:\n",
        "                continue\n",
        "            center_word_index = subsampled_vocab[center_word]\n",
        "            context = []\n",
        "            for j in range(i - w2v.window_size, i + w2v.window_size + 1):\n",
        "                if i != j and 0 <= j < len(sentence) and sentence[j] in subsampled_vocab:\n",
        "                    context.append(subsampled_vocab[sentence[j]])\n",
        "            negative_samples_indices = np.random.choice(sampling_table, size=negative_samples)\n",
        "            w2v.X_train.append(center_word_index)\n",
        "            w2v.y_train.append(context)\n",
        "            w2v.y_train.extend(negative_samples_indices)\n",
        "\n",
        "    w2v.initialize(V, data)\n",
        "\n",
        "    return w2v.X_train, w2v.y_train\n"
      ],
      "metadata": {
        "id": "zdDNv3tOoILS"
      },
      "id": "zdDNv3tOoILS",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\n",
        "corpus += \"The earth revolves around the sun. The moon revolves around the earth\"\n",
        "epochs = 1000\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n",
        "\n",
        "print(w2v.predict(\"around\",3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_7ljdqeuUcx",
        "outputId": "60bb18b9-d00a-4987-8c3f-2c1966cf6bd6"
      },
      "id": "i_7ljdqeuUcx",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  0\n",
            "epoch  2  loss =  0\n",
            "epoch  3  loss =  0\n",
            "epoch  4  loss =  0\n",
            "epoch  5  loss =  0\n",
            "epoch  6  loss =  0\n",
            "epoch  7  loss =  0\n",
            "epoch  8  loss =  0\n",
            "epoch  9  loss =  0\n",
            "epoch  10  loss =  0\n",
            "epoch  11  loss =  0\n",
            "epoch  12  loss =  0\n",
            "epoch  13  loss =  0\n",
            "epoch  14  loss =  0\n",
            "epoch  15  loss =  0\n",
            "epoch  16  loss =  0\n",
            "epoch  17  loss =  0\n",
            "epoch  18  loss =  0\n",
            "epoch  19  loss =  0\n",
            "epoch  20  loss =  0\n",
            "epoch  21  loss =  0\n",
            "epoch  22  loss =  0\n",
            "epoch  23  loss =  0\n",
            "epoch  24  loss =  0\n",
            "epoch  25  loss =  0\n",
            "epoch  26  loss =  0\n",
            "epoch  27  loss =  0\n",
            "epoch  28  loss =  0\n",
            "epoch  29  loss =  0\n",
            "epoch  30  loss =  0\n",
            "epoch  31  loss =  0\n",
            "epoch  32  loss =  0\n",
            "epoch  33  loss =  0\n",
            "epoch  34  loss =  0\n",
            "epoch  35  loss =  0\n",
            "epoch  36  loss =  0\n",
            "epoch  37  loss =  0\n",
            "epoch  38  loss =  0\n",
            "epoch  39  loss =  0\n",
            "epoch  40  loss =  0\n",
            "epoch  41  loss =  0\n",
            "epoch  42  loss =  0\n",
            "epoch  43  loss =  0\n",
            "epoch  44  loss =  0\n",
            "epoch  45  loss =  0\n",
            "epoch  46  loss =  0\n",
            "epoch  47  loss =  0\n",
            "epoch  48  loss =  0\n",
            "epoch  49  loss =  0\n",
            "epoch  50  loss =  0\n",
            "epoch  51  loss =  0\n",
            "epoch  52  loss =  0\n",
            "epoch  53  loss =  0\n",
            "epoch  54  loss =  0\n",
            "epoch  55  loss =  0\n",
            "epoch  56  loss =  0\n",
            "epoch  57  loss =  0\n",
            "epoch  58  loss =  0\n",
            "epoch  59  loss =  0\n",
            "epoch  60  loss =  0\n",
            "epoch  61  loss =  0\n",
            "epoch  62  loss =  0\n",
            "epoch  63  loss =  0\n",
            "epoch  64  loss =  0\n",
            "epoch  65  loss =  0\n",
            "epoch  66  loss =  0\n",
            "epoch  67  loss =  0\n",
            "epoch  68  loss =  0\n",
            "epoch  69  loss =  0\n",
            "epoch  70  loss =  0\n",
            "epoch  71  loss =  0\n",
            "epoch  72  loss =  0\n",
            "epoch  73  loss =  0\n",
            "epoch  74  loss =  0\n",
            "epoch  75  loss =  0\n",
            "epoch  76  loss =  0\n",
            "epoch  77  loss =  0\n",
            "epoch  78  loss =  0\n",
            "epoch  79  loss =  0\n",
            "epoch  80  loss =  0\n",
            "epoch  81  loss =  0\n",
            "epoch  82  loss =  0\n",
            "epoch  83  loss =  0\n",
            "epoch  84  loss =  0\n",
            "epoch  85  loss =  0\n",
            "epoch  86  loss =  0\n",
            "epoch  87  loss =  0\n",
            "epoch  88  loss =  0\n",
            "epoch  89  loss =  0\n",
            "epoch  90  loss =  0\n",
            "epoch  91  loss =  0\n",
            "epoch  92  loss =  0\n",
            "epoch  93  loss =  0\n",
            "epoch  94  loss =  0\n",
            "epoch  95  loss =  0\n",
            "epoch  96  loss =  0\n",
            "epoch  97  loss =  0\n",
            "epoch  98  loss =  0\n",
            "epoch  99  loss =  0\n",
            "epoch  100  loss =  0\n",
            "epoch  101  loss =  0\n",
            "epoch  102  loss =  0\n",
            "epoch  103  loss =  0\n",
            "epoch  104  loss =  0\n",
            "epoch  105  loss =  0\n",
            "epoch  106  loss =  0\n",
            "epoch  107  loss =  0\n",
            "epoch  108  loss =  0\n",
            "epoch  109  loss =  0\n",
            "epoch  110  loss =  0\n",
            "epoch  111  loss =  0\n",
            "epoch  112  loss =  0\n",
            "epoch  113  loss =  0\n",
            "epoch  114  loss =  0\n",
            "epoch  115  loss =  0\n",
            "epoch  116  loss =  0\n",
            "epoch  117  loss =  0\n",
            "epoch  118  loss =  0\n",
            "epoch  119  loss =  0\n",
            "epoch  120  loss =  0\n",
            "epoch  121  loss =  0\n",
            "epoch  122  loss =  0\n",
            "epoch  123  loss =  0\n",
            "epoch  124  loss =  0\n",
            "epoch  125  loss =  0\n",
            "epoch  126  loss =  0\n",
            "epoch  127  loss =  0\n",
            "epoch  128  loss =  0\n",
            "epoch  129  loss =  0\n",
            "epoch  130  loss =  0\n",
            "epoch  131  loss =  0\n",
            "epoch  132  loss =  0\n",
            "epoch  133  loss =  0\n",
            "epoch  134  loss =  0\n",
            "epoch  135  loss =  0\n",
            "epoch  136  loss =  0\n",
            "epoch  137  loss =  0\n",
            "epoch  138  loss =  0\n",
            "epoch  139  loss =  0\n",
            "epoch  140  loss =  0\n",
            "epoch  141  loss =  0\n",
            "epoch  142  loss =  0\n",
            "epoch  143  loss =  0\n",
            "epoch  144  loss =  0\n",
            "epoch  145  loss =  0\n",
            "epoch  146  loss =  0\n",
            "epoch  147  loss =  0\n",
            "epoch  148  loss =  0\n",
            "epoch  149  loss =  0\n",
            "epoch  150  loss =  0\n",
            "epoch  151  loss =  0\n",
            "epoch  152  loss =  0\n",
            "epoch  153  loss =  0\n",
            "epoch  154  loss =  0\n",
            "epoch  155  loss =  0\n",
            "epoch  156  loss =  0\n",
            "epoch  157  loss =  0\n",
            "epoch  158  loss =  0\n",
            "epoch  159  loss =  0\n",
            "epoch  160  loss =  0\n",
            "epoch  161  loss =  0\n",
            "epoch  162  loss =  0\n",
            "epoch  163  loss =  0\n",
            "epoch  164  loss =  0\n",
            "epoch  165  loss =  0\n",
            "epoch  166  loss =  0\n",
            "epoch  167  loss =  0\n",
            "epoch  168  loss =  0\n",
            "epoch  169  loss =  0\n",
            "epoch  170  loss =  0\n",
            "epoch  171  loss =  0\n",
            "epoch  172  loss =  0\n",
            "epoch  173  loss =  0\n",
            "epoch  174  loss =  0\n",
            "epoch  175  loss =  0\n",
            "epoch  176  loss =  0\n",
            "epoch  177  loss =  0\n",
            "epoch  178  loss =  0\n",
            "epoch  179  loss =  0\n",
            "epoch  180  loss =  0\n",
            "epoch  181  loss =  0\n",
            "epoch  182  loss =  0\n",
            "epoch  183  loss =  0\n",
            "epoch  184  loss =  0\n",
            "epoch  185  loss =  0\n",
            "epoch  186  loss =  0\n",
            "epoch  187  loss =  0\n",
            "epoch  188  loss =  0\n",
            "epoch  189  loss =  0\n",
            "epoch  190  loss =  0\n",
            "epoch  191  loss =  0\n",
            "epoch  192  loss =  0\n",
            "epoch  193  loss =  0\n",
            "epoch  194  loss =  0\n",
            "epoch  195  loss =  0\n",
            "epoch  196  loss =  0\n",
            "epoch  197  loss =  0\n",
            "epoch  198  loss =  0\n",
            "epoch  199  loss =  0\n",
            "epoch  200  loss =  0\n",
            "epoch  201  loss =  0\n",
            "epoch  202  loss =  0\n",
            "epoch  203  loss =  0\n",
            "epoch  204  loss =  0\n",
            "epoch  205  loss =  0\n",
            "epoch  206  loss =  0\n",
            "epoch  207  loss =  0\n",
            "epoch  208  loss =  0\n",
            "epoch  209  loss =  0\n",
            "epoch  210  loss =  0\n",
            "epoch  211  loss =  0\n",
            "epoch  212  loss =  0\n",
            "epoch  213  loss =  0\n",
            "epoch  214  loss =  0\n",
            "epoch  215  loss =  0\n",
            "epoch  216  loss =  0\n",
            "epoch  217  loss =  0\n",
            "epoch  218  loss =  0\n",
            "epoch  219  loss =  0\n",
            "epoch  220  loss =  0\n",
            "epoch  221  loss =  0\n",
            "epoch  222  loss =  0\n",
            "epoch  223  loss =  0\n",
            "epoch  224  loss =  0\n",
            "epoch  225  loss =  0\n",
            "epoch  226  loss =  0\n",
            "epoch  227  loss =  0\n",
            "epoch  228  loss =  0\n",
            "epoch  229  loss =  0\n",
            "epoch  230  loss =  0\n",
            "epoch  231  loss =  0\n",
            "epoch  232  loss =  0\n",
            "epoch  233  loss =  0\n",
            "epoch  234  loss =  0\n",
            "epoch  235  loss =  0\n",
            "epoch  236  loss =  0\n",
            "epoch  237  loss =  0\n",
            "epoch  238  loss =  0\n",
            "epoch  239  loss =  0\n",
            "epoch  240  loss =  0\n",
            "epoch  241  loss =  0\n",
            "epoch  242  loss =  0\n",
            "epoch  243  loss =  0\n",
            "epoch  244  loss =  0\n",
            "epoch  245  loss =  0\n",
            "epoch  246  loss =  0\n",
            "epoch  247  loss =  0\n",
            "epoch  248  loss =  0\n",
            "epoch  249  loss =  0\n",
            "epoch  250  loss =  0\n",
            "epoch  251  loss =  0\n",
            "epoch  252  loss =  0\n",
            "epoch  253  loss =  0\n",
            "epoch  254  loss =  0\n",
            "epoch  255  loss =  0\n",
            "epoch  256  loss =  0\n",
            "epoch  257  loss =  0\n",
            "epoch  258  loss =  0\n",
            "epoch  259  loss =  0\n",
            "epoch  260  loss =  0\n",
            "epoch  261  loss =  0\n",
            "epoch  262  loss =  0\n",
            "epoch  263  loss =  0\n",
            "epoch  264  loss =  0\n",
            "epoch  265  loss =  0\n",
            "epoch  266  loss =  0\n",
            "epoch  267  loss =  0\n",
            "epoch  268  loss =  0\n",
            "epoch  269  loss =  0\n",
            "epoch  270  loss =  0\n",
            "epoch  271  loss =  0\n",
            "epoch  272  loss =  0\n",
            "epoch  273  loss =  0\n",
            "epoch  274  loss =  0\n",
            "epoch  275  loss =  0\n",
            "epoch  276  loss =  0\n",
            "epoch  277  loss =  0\n",
            "epoch  278  loss =  0\n",
            "epoch  279  loss =  0\n",
            "epoch  280  loss =  0\n",
            "epoch  281  loss =  0\n",
            "epoch  282  loss =  0\n",
            "epoch  283  loss =  0\n",
            "epoch  284  loss =  0\n",
            "epoch  285  loss =  0\n",
            "epoch  286  loss =  0\n",
            "epoch  287  loss =  0\n",
            "epoch  288  loss =  0\n",
            "epoch  289  loss =  0\n",
            "epoch  290  loss =  0\n",
            "epoch  291  loss =  0\n",
            "epoch  292  loss =  0\n",
            "epoch  293  loss =  0\n",
            "epoch  294  loss =  0\n",
            "epoch  295  loss =  0\n",
            "epoch  296  loss =  0\n",
            "epoch  297  loss =  0\n",
            "epoch  298  loss =  0\n",
            "epoch  299  loss =  0\n",
            "epoch  300  loss =  0\n",
            "epoch  301  loss =  0\n",
            "epoch  302  loss =  0\n",
            "epoch  303  loss =  0\n",
            "epoch  304  loss =  0\n",
            "epoch  305  loss =  0\n",
            "epoch  306  loss =  0\n",
            "epoch  307  loss =  0\n",
            "epoch  308  loss =  0\n",
            "epoch  309  loss =  0\n",
            "epoch  310  loss =  0\n",
            "epoch  311  loss =  0\n",
            "epoch  312  loss =  0\n",
            "epoch  313  loss =  0\n",
            "epoch  314  loss =  0\n",
            "epoch  315  loss =  0\n",
            "epoch  316  loss =  0\n",
            "epoch  317  loss =  0\n",
            "epoch  318  loss =  0\n",
            "epoch  319  loss =  0\n",
            "epoch  320  loss =  0\n",
            "epoch  321  loss =  0\n",
            "epoch  322  loss =  0\n",
            "epoch  323  loss =  0\n",
            "epoch  324  loss =  0\n",
            "epoch  325  loss =  0\n",
            "epoch  326  loss =  0\n",
            "epoch  327  loss =  0\n",
            "epoch  328  loss =  0\n",
            "epoch  329  loss =  0\n",
            "epoch  330  loss =  0\n",
            "epoch  331  loss =  0\n",
            "epoch  332  loss =  0\n",
            "epoch  333  loss =  0\n",
            "epoch  334  loss =  0\n",
            "epoch  335  loss =  0\n",
            "epoch  336  loss =  0\n",
            "epoch  337  loss =  0\n",
            "epoch  338  loss =  0\n",
            "epoch  339  loss =  0\n",
            "epoch  340  loss =  0\n",
            "epoch  341  loss =  0\n",
            "epoch  342  loss =  0\n",
            "epoch  343  loss =  0\n",
            "epoch  344  loss =  0\n",
            "epoch  345  loss =  0\n",
            "epoch  346  loss =  0\n",
            "epoch  347  loss =  0\n",
            "epoch  348  loss =  0\n",
            "epoch  349  loss =  0\n",
            "epoch  350  loss =  0\n",
            "epoch  351  loss =  0\n",
            "epoch  352  loss =  0\n",
            "epoch  353  loss =  0\n",
            "epoch  354  loss =  0\n",
            "epoch  355  loss =  0\n",
            "epoch  356  loss =  0\n",
            "epoch  357  loss =  0\n",
            "epoch  358  loss =  0\n",
            "epoch  359  loss =  0\n",
            "epoch  360  loss =  0\n",
            "epoch  361  loss =  0\n",
            "epoch  362  loss =  0\n",
            "epoch  363  loss =  0\n",
            "epoch  364  loss =  0\n",
            "epoch  365  loss =  0\n",
            "epoch  366  loss =  0\n",
            "epoch  367  loss =  0\n",
            "epoch  368  loss =  0\n",
            "epoch  369  loss =  0\n",
            "epoch  370  loss =  0\n",
            "epoch  371  loss =  0\n",
            "epoch  372  loss =  0\n",
            "epoch  373  loss =  0\n",
            "epoch  374  loss =  0\n",
            "epoch  375  loss =  0\n",
            "epoch  376  loss =  0\n",
            "epoch  377  loss =  0\n",
            "epoch  378  loss =  0\n",
            "epoch  379  loss =  0\n",
            "epoch  380  loss =  0\n",
            "epoch  381  loss =  0\n",
            "epoch  382  loss =  0\n",
            "epoch  383  loss =  0\n",
            "epoch  384  loss =  0\n",
            "epoch  385  loss =  0\n",
            "epoch  386  loss =  0\n",
            "epoch  387  loss =  0\n",
            "epoch  388  loss =  0\n",
            "epoch  389  loss =  0\n",
            "epoch  390  loss =  0\n",
            "epoch  391  loss =  0\n",
            "epoch  392  loss =  0\n",
            "epoch  393  loss =  0\n",
            "epoch  394  loss =  0\n",
            "epoch  395  loss =  0\n",
            "epoch  396  loss =  0\n",
            "epoch  397  loss =  0\n",
            "epoch  398  loss =  0\n",
            "epoch  399  loss =  0\n",
            "epoch  400  loss =  0\n",
            "epoch  401  loss =  0\n",
            "epoch  402  loss =  0\n",
            "epoch  403  loss =  0\n",
            "epoch  404  loss =  0\n",
            "epoch  405  loss =  0\n",
            "epoch  406  loss =  0\n",
            "epoch  407  loss =  0\n",
            "epoch  408  loss =  0\n",
            "epoch  409  loss =  0\n",
            "epoch  410  loss =  0\n",
            "epoch  411  loss =  0\n",
            "epoch  412  loss =  0\n",
            "epoch  413  loss =  0\n",
            "epoch  414  loss =  0\n",
            "epoch  415  loss =  0\n",
            "epoch  416  loss =  0\n",
            "epoch  417  loss =  0\n",
            "epoch  418  loss =  0\n",
            "epoch  419  loss =  0\n",
            "epoch  420  loss =  0\n",
            "epoch  421  loss =  0\n",
            "epoch  422  loss =  0\n",
            "epoch  423  loss =  0\n",
            "epoch  424  loss =  0\n",
            "epoch  425  loss =  0\n",
            "epoch  426  loss =  0\n",
            "epoch  427  loss =  0\n",
            "epoch  428  loss =  0\n",
            "epoch  429  loss =  0\n",
            "epoch  430  loss =  0\n",
            "epoch  431  loss =  0\n",
            "epoch  432  loss =  0\n",
            "epoch  433  loss =  0\n",
            "epoch  434  loss =  0\n",
            "epoch  435  loss =  0\n",
            "epoch  436  loss =  0\n",
            "epoch  437  loss =  0\n",
            "epoch  438  loss =  0\n",
            "epoch  439  loss =  0\n",
            "epoch  440  loss =  0\n",
            "epoch  441  loss =  0\n",
            "epoch  442  loss =  0\n",
            "epoch  443  loss =  0\n",
            "epoch  444  loss =  0\n",
            "epoch  445  loss =  0\n",
            "epoch  446  loss =  0\n",
            "epoch  447  loss =  0\n",
            "epoch  448  loss =  0\n",
            "epoch  449  loss =  0\n",
            "epoch  450  loss =  0\n",
            "epoch  451  loss =  0\n",
            "epoch  452  loss =  0\n",
            "epoch  453  loss =  0\n",
            "epoch  454  loss =  0\n",
            "epoch  455  loss =  0\n",
            "epoch  456  loss =  0\n",
            "epoch  457  loss =  0\n",
            "epoch  458  loss =  0\n",
            "epoch  459  loss =  0\n",
            "epoch  460  loss =  0\n",
            "epoch  461  loss =  0\n",
            "epoch  462  loss =  0\n",
            "epoch  463  loss =  0\n",
            "epoch  464  loss =  0\n",
            "epoch  465  loss =  0\n",
            "epoch  466  loss =  0\n",
            "epoch  467  loss =  0\n",
            "epoch  468  loss =  0\n",
            "epoch  469  loss =  0\n",
            "epoch  470  loss =  0\n",
            "epoch  471  loss =  0\n",
            "epoch  472  loss =  0\n",
            "epoch  473  loss =  0\n",
            "epoch  474  loss =  0\n",
            "epoch  475  loss =  0\n",
            "epoch  476  loss =  0\n",
            "epoch  477  loss =  0\n",
            "epoch  478  loss =  0\n",
            "epoch  479  loss =  0\n",
            "epoch  480  loss =  0\n",
            "epoch  481  loss =  0\n",
            "epoch  482  loss =  0\n",
            "epoch  483  loss =  0\n",
            "epoch  484  loss =  0\n",
            "epoch  485  loss =  0\n",
            "epoch  486  loss =  0\n",
            "epoch  487  loss =  0\n",
            "epoch  488  loss =  0\n",
            "epoch  489  loss =  0\n",
            "epoch  490  loss =  0\n",
            "epoch  491  loss =  0\n",
            "epoch  492  loss =  0\n",
            "epoch  493  loss =  0\n",
            "epoch  494  loss =  0\n",
            "epoch  495  loss =  0\n",
            "epoch  496  loss =  0\n",
            "epoch  497  loss =  0\n",
            "epoch  498  loss =  0\n",
            "epoch  499  loss =  0\n",
            "epoch  500  loss =  0\n",
            "epoch  501  loss =  0\n",
            "epoch  502  loss =  0\n",
            "epoch  503  loss =  0\n",
            "epoch  504  loss =  0\n",
            "epoch  505  loss =  0\n",
            "epoch  506  loss =  0\n",
            "epoch  507  loss =  0\n",
            "epoch  508  loss =  0\n",
            "epoch  509  loss =  0\n",
            "epoch  510  loss =  0\n",
            "epoch  511  loss =  0\n",
            "epoch  512  loss =  0\n",
            "epoch  513  loss =  0\n",
            "epoch  514  loss =  0\n",
            "epoch  515  loss =  0\n",
            "epoch  516  loss =  0\n",
            "epoch  517  loss =  0\n",
            "epoch  518  loss =  0\n",
            "epoch  519  loss =  0\n",
            "epoch  520  loss =  0\n",
            "epoch  521  loss =  0\n",
            "epoch  522  loss =  0\n",
            "epoch  523  loss =  0\n",
            "epoch  524  loss =  0\n",
            "epoch  525  loss =  0\n",
            "epoch  526  loss =  0\n",
            "epoch  527  loss =  0\n",
            "epoch  528  loss =  0\n",
            "epoch  529  loss =  0\n",
            "epoch  530  loss =  0\n",
            "epoch  531  loss =  0\n",
            "epoch  532  loss =  0\n",
            "epoch  533  loss =  0\n",
            "epoch  534  loss =  0\n",
            "epoch  535  loss =  0\n",
            "epoch  536  loss =  0\n",
            "epoch  537  loss =  0\n",
            "epoch  538  loss =  0\n",
            "epoch  539  loss =  0\n",
            "epoch  540  loss =  0\n",
            "epoch  541  loss =  0\n",
            "epoch  542  loss =  0\n",
            "epoch  543  loss =  0\n",
            "epoch  544  loss =  0\n",
            "epoch  545  loss =  0\n",
            "epoch  546  loss =  0\n",
            "epoch  547  loss =  0\n",
            "epoch  548  loss =  0\n",
            "epoch  549  loss =  0\n",
            "epoch  550  loss =  0\n",
            "epoch  551  loss =  0\n",
            "epoch  552  loss =  0\n",
            "epoch  553  loss =  0\n",
            "epoch  554  loss =  0\n",
            "epoch  555  loss =  0\n",
            "epoch  556  loss =  0\n",
            "epoch  557  loss =  0\n",
            "epoch  558  loss =  0\n",
            "epoch  559  loss =  0\n",
            "epoch  560  loss =  0\n",
            "epoch  561  loss =  0\n",
            "epoch  562  loss =  0\n",
            "epoch  563  loss =  0\n",
            "epoch  564  loss =  0\n",
            "epoch  565  loss =  0\n",
            "epoch  566  loss =  0\n",
            "epoch  567  loss =  0\n",
            "epoch  568  loss =  0\n",
            "epoch  569  loss =  0\n",
            "epoch  570  loss =  0\n",
            "epoch  571  loss =  0\n",
            "epoch  572  loss =  0\n",
            "epoch  573  loss =  0\n",
            "epoch  574  loss =  0\n",
            "epoch  575  loss =  0\n",
            "epoch  576  loss =  0\n",
            "epoch  577  loss =  0\n",
            "epoch  578  loss =  0\n",
            "epoch  579  loss =  0\n",
            "epoch  580  loss =  0\n",
            "epoch  581  loss =  0\n",
            "epoch  582  loss =  0\n",
            "epoch  583  loss =  0\n",
            "epoch  584  loss =  0\n",
            "epoch  585  loss =  0\n",
            "epoch  586  loss =  0\n",
            "epoch  587  loss =  0\n",
            "epoch  588  loss =  0\n",
            "epoch  589  loss =  0\n",
            "epoch  590  loss =  0\n",
            "epoch  591  loss =  0\n",
            "epoch  592  loss =  0\n",
            "epoch  593  loss =  0\n",
            "epoch  594  loss =  0\n",
            "epoch  595  loss =  0\n",
            "epoch  596  loss =  0\n",
            "epoch  597  loss =  0\n",
            "epoch  598  loss =  0\n",
            "epoch  599  loss =  0\n",
            "epoch  600  loss =  0\n",
            "epoch  601  loss =  0\n",
            "epoch  602  loss =  0\n",
            "epoch  603  loss =  0\n",
            "epoch  604  loss =  0\n",
            "epoch  605  loss =  0\n",
            "epoch  606  loss =  0\n",
            "epoch  607  loss =  0\n",
            "epoch  608  loss =  0\n",
            "epoch  609  loss =  0\n",
            "epoch  610  loss =  0\n",
            "epoch  611  loss =  0\n",
            "epoch  612  loss =  0\n",
            "epoch  613  loss =  0\n",
            "epoch  614  loss =  0\n",
            "epoch  615  loss =  0\n",
            "epoch  616  loss =  0\n",
            "epoch  617  loss =  0\n",
            "epoch  618  loss =  0\n",
            "epoch  619  loss =  0\n",
            "epoch  620  loss =  0\n",
            "epoch  621  loss =  0\n",
            "epoch  622  loss =  0\n",
            "epoch  623  loss =  0\n",
            "epoch  624  loss =  0\n",
            "epoch  625  loss =  0\n",
            "epoch  626  loss =  0\n",
            "epoch  627  loss =  0\n",
            "epoch  628  loss =  0\n",
            "epoch  629  loss =  0\n",
            "epoch  630  loss =  0\n",
            "epoch  631  loss =  0\n",
            "epoch  632  loss =  0\n",
            "epoch  633  loss =  0\n",
            "epoch  634  loss =  0\n",
            "epoch  635  loss =  0\n",
            "epoch  636  loss =  0\n",
            "epoch  637  loss =  0\n",
            "epoch  638  loss =  0\n",
            "epoch  639  loss =  0\n",
            "epoch  640  loss =  0\n",
            "epoch  641  loss =  0\n",
            "epoch  642  loss =  0\n",
            "epoch  643  loss =  0\n",
            "epoch  644  loss =  0\n",
            "epoch  645  loss =  0\n",
            "epoch  646  loss =  0\n",
            "epoch  647  loss =  0\n",
            "epoch  648  loss =  0\n",
            "epoch  649  loss =  0\n",
            "epoch  650  loss =  0\n",
            "epoch  651  loss =  0\n",
            "epoch  652  loss =  0\n",
            "epoch  653  loss =  0\n",
            "epoch  654  loss =  0\n",
            "epoch  655  loss =  0\n",
            "epoch  656  loss =  0\n",
            "epoch  657  loss =  0\n",
            "epoch  658  loss =  0\n",
            "epoch  659  loss =  0\n",
            "epoch  660  loss =  0\n",
            "epoch  661  loss =  0\n",
            "epoch  662  loss =  0\n",
            "epoch  663  loss =  0\n",
            "epoch  664  loss =  0\n",
            "epoch  665  loss =  0\n",
            "epoch  666  loss =  0\n",
            "epoch  667  loss =  0\n",
            "epoch  668  loss =  0\n",
            "epoch  669  loss =  0\n",
            "epoch  670  loss =  0\n",
            "epoch  671  loss =  0\n",
            "epoch  672  loss =  0\n",
            "epoch  673  loss =  0\n",
            "epoch  674  loss =  0\n",
            "epoch  675  loss =  0\n",
            "epoch  676  loss =  0\n",
            "epoch  677  loss =  0\n",
            "epoch  678  loss =  0\n",
            "epoch  679  loss =  0\n",
            "epoch  680  loss =  0\n",
            "epoch  681  loss =  0\n",
            "epoch  682  loss =  0\n",
            "epoch  683  loss =  0\n",
            "epoch  684  loss =  0\n",
            "epoch  685  loss =  0\n",
            "epoch  686  loss =  0\n",
            "epoch  687  loss =  0\n",
            "epoch  688  loss =  0\n",
            "epoch  689  loss =  0\n",
            "epoch  690  loss =  0\n",
            "epoch  691  loss =  0\n",
            "epoch  692  loss =  0\n",
            "epoch  693  loss =  0\n",
            "epoch  694  loss =  0\n",
            "epoch  695  loss =  0\n",
            "epoch  696  loss =  0\n",
            "epoch  697  loss =  0\n",
            "epoch  698  loss =  0\n",
            "epoch  699  loss =  0\n",
            "epoch  700  loss =  0\n",
            "epoch  701  loss =  0\n",
            "epoch  702  loss =  0\n",
            "epoch  703  loss =  0\n",
            "epoch  704  loss =  0\n",
            "epoch  705  loss =  0\n",
            "epoch  706  loss =  0\n",
            "epoch  707  loss =  0\n",
            "epoch  708  loss =  0\n",
            "epoch  709  loss =  0\n",
            "epoch  710  loss =  0\n",
            "epoch  711  loss =  0\n",
            "epoch  712  loss =  0\n",
            "epoch  713  loss =  0\n",
            "epoch  714  loss =  0\n",
            "epoch  715  loss =  0\n",
            "epoch  716  loss =  0\n",
            "epoch  717  loss =  0\n",
            "epoch  718  loss =  0\n",
            "epoch  719  loss =  0\n",
            "epoch  720  loss =  0\n",
            "epoch  721  loss =  0\n",
            "epoch  722  loss =  0\n",
            "epoch  723  loss =  0\n",
            "epoch  724  loss =  0\n",
            "epoch  725  loss =  0\n",
            "epoch  726  loss =  0\n",
            "epoch  727  loss =  0\n",
            "epoch  728  loss =  0\n",
            "epoch  729  loss =  0\n",
            "epoch  730  loss =  0\n",
            "epoch  731  loss =  0\n",
            "epoch  732  loss =  0\n",
            "epoch  733  loss =  0\n",
            "epoch  734  loss =  0\n",
            "epoch  735  loss =  0\n",
            "epoch  736  loss =  0\n",
            "epoch  737  loss =  0\n",
            "epoch  738  loss =  0\n",
            "epoch  739  loss =  0\n",
            "epoch  740  loss =  0\n",
            "epoch  741  loss =  0\n",
            "epoch  742  loss =  0\n",
            "epoch  743  loss =  0\n",
            "epoch  744  loss =  0\n",
            "epoch  745  loss =  0\n",
            "epoch  746  loss =  0\n",
            "epoch  747  loss =  0\n",
            "epoch  748  loss =  0\n",
            "epoch  749  loss =  0\n",
            "epoch  750  loss =  0\n",
            "epoch  751  loss =  0\n",
            "epoch  752  loss =  0\n",
            "epoch  753  loss =  0\n",
            "epoch  754  loss =  0\n",
            "epoch  755  loss =  0\n",
            "epoch  756  loss =  0\n",
            "epoch  757  loss =  0\n",
            "epoch  758  loss =  0\n",
            "epoch  759  loss =  0\n",
            "epoch  760  loss =  0\n",
            "epoch  761  loss =  0\n",
            "epoch  762  loss =  0\n",
            "epoch  763  loss =  0\n",
            "epoch  764  loss =  0\n",
            "epoch  765  loss =  0\n",
            "epoch  766  loss =  0\n",
            "epoch  767  loss =  0\n",
            "epoch  768  loss =  0\n",
            "epoch  769  loss =  0\n",
            "epoch  770  loss =  0\n",
            "epoch  771  loss =  0\n",
            "epoch  772  loss =  0\n",
            "epoch  773  loss =  0\n",
            "epoch  774  loss =  0\n",
            "epoch  775  loss =  0\n",
            "epoch  776  loss =  0\n",
            "epoch  777  loss =  0\n",
            "epoch  778  loss =  0\n",
            "epoch  779  loss =  0\n",
            "epoch  780  loss =  0\n",
            "epoch  781  loss =  0\n",
            "epoch  782  loss =  0\n",
            "epoch  783  loss =  0\n",
            "epoch  784  loss =  0\n",
            "epoch  785  loss =  0\n",
            "epoch  786  loss =  0\n",
            "epoch  787  loss =  0\n",
            "epoch  788  loss =  0\n",
            "epoch  789  loss =  0\n",
            "epoch  790  loss =  0\n",
            "epoch  791  loss =  0\n",
            "epoch  792  loss =  0\n",
            "epoch  793  loss =  0\n",
            "epoch  794  loss =  0\n",
            "epoch  795  loss =  0\n",
            "epoch  796  loss =  0\n",
            "epoch  797  loss =  0\n",
            "epoch  798  loss =  0\n",
            "epoch  799  loss =  0\n",
            "epoch  800  loss =  0\n",
            "epoch  801  loss =  0\n",
            "epoch  802  loss =  0\n",
            "epoch  803  loss =  0\n",
            "epoch  804  loss =  0\n",
            "epoch  805  loss =  0\n",
            "epoch  806  loss =  0\n",
            "epoch  807  loss =  0\n",
            "epoch  808  loss =  0\n",
            "epoch  809  loss =  0\n",
            "epoch  810  loss =  0\n",
            "epoch  811  loss =  0\n",
            "epoch  812  loss =  0\n",
            "epoch  813  loss =  0\n",
            "epoch  814  loss =  0\n",
            "epoch  815  loss =  0\n",
            "epoch  816  loss =  0\n",
            "epoch  817  loss =  0\n",
            "epoch  818  loss =  0\n",
            "epoch  819  loss =  0\n",
            "epoch  820  loss =  0\n",
            "epoch  821  loss =  0\n",
            "epoch  822  loss =  0\n",
            "epoch  823  loss =  0\n",
            "epoch  824  loss =  0\n",
            "epoch  825  loss =  0\n",
            "epoch  826  loss =  0\n",
            "epoch  827  loss =  0\n",
            "epoch  828  loss =  0\n",
            "epoch  829  loss =  0\n",
            "epoch  830  loss =  0\n",
            "epoch  831  loss =  0\n",
            "epoch  832  loss =  0\n",
            "epoch  833  loss =  0\n",
            "epoch  834  loss =  0\n",
            "epoch  835  loss =  0\n",
            "epoch  836  loss =  0\n",
            "epoch  837  loss =  0\n",
            "epoch  838  loss =  0\n",
            "epoch  839  loss =  0\n",
            "epoch  840  loss =  0\n",
            "epoch  841  loss =  0\n",
            "epoch  842  loss =  0\n",
            "epoch  843  loss =  0\n",
            "epoch  844  loss =  0\n",
            "epoch  845  loss =  0\n",
            "epoch  846  loss =  0\n",
            "epoch  847  loss =  0\n",
            "epoch  848  loss =  0\n",
            "epoch  849  loss =  0\n",
            "epoch  850  loss =  0\n",
            "epoch  851  loss =  0\n",
            "epoch  852  loss =  0\n",
            "epoch  853  loss =  0\n",
            "epoch  854  loss =  0\n",
            "epoch  855  loss =  0\n",
            "epoch  856  loss =  0\n",
            "epoch  857  loss =  0\n",
            "epoch  858  loss =  0\n",
            "epoch  859  loss =  0\n",
            "epoch  860  loss =  0\n",
            "epoch  861  loss =  0\n",
            "epoch  862  loss =  0\n",
            "epoch  863  loss =  0\n",
            "epoch  864  loss =  0\n",
            "epoch  865  loss =  0\n",
            "epoch  866  loss =  0\n",
            "epoch  867  loss =  0\n",
            "epoch  868  loss =  0\n",
            "epoch  869  loss =  0\n",
            "epoch  870  loss =  0\n",
            "epoch  871  loss =  0\n",
            "epoch  872  loss =  0\n",
            "epoch  873  loss =  0\n",
            "epoch  874  loss =  0\n",
            "epoch  875  loss =  0\n",
            "epoch  876  loss =  0\n",
            "epoch  877  loss =  0\n",
            "epoch  878  loss =  0\n",
            "epoch  879  loss =  0\n",
            "epoch  880  loss =  0\n",
            "epoch  881  loss =  0\n",
            "epoch  882  loss =  0\n",
            "epoch  883  loss =  0\n",
            "epoch  884  loss =  0\n",
            "epoch  885  loss =  0\n",
            "epoch  886  loss =  0\n",
            "epoch  887  loss =  0\n",
            "epoch  888  loss =  0\n",
            "epoch  889  loss =  0\n",
            "epoch  890  loss =  0\n",
            "epoch  891  loss =  0\n",
            "epoch  892  loss =  0\n",
            "epoch  893  loss =  0\n",
            "epoch  894  loss =  0\n",
            "epoch  895  loss =  0\n",
            "epoch  896  loss =  0\n",
            "epoch  897  loss =  0\n",
            "epoch  898  loss =  0\n",
            "epoch  899  loss =  0\n",
            "epoch  900  loss =  0\n",
            "epoch  901  loss =  0\n",
            "epoch  902  loss =  0\n",
            "epoch  903  loss =  0\n",
            "epoch  904  loss =  0\n",
            "epoch  905  loss =  0\n",
            "epoch  906  loss =  0\n",
            "epoch  907  loss =  0\n",
            "epoch  908  loss =  0\n",
            "epoch  909  loss =  0\n",
            "epoch  910  loss =  0\n",
            "epoch  911  loss =  0\n",
            "epoch  912  loss =  0\n",
            "epoch  913  loss =  0\n",
            "epoch  914  loss =  0\n",
            "epoch  915  loss =  0\n",
            "epoch  916  loss =  0\n",
            "epoch  917  loss =  0\n",
            "epoch  918  loss =  0\n",
            "epoch  919  loss =  0\n",
            "epoch  920  loss =  0\n",
            "epoch  921  loss =  0\n",
            "epoch  922  loss =  0\n",
            "epoch  923  loss =  0\n",
            "epoch  924  loss =  0\n",
            "epoch  925  loss =  0\n",
            "epoch  926  loss =  0\n",
            "epoch  927  loss =  0\n",
            "epoch  928  loss =  0\n",
            "epoch  929  loss =  0\n",
            "epoch  930  loss =  0\n",
            "epoch  931  loss =  0\n",
            "epoch  932  loss =  0\n",
            "epoch  933  loss =  0\n",
            "epoch  934  loss =  0\n",
            "epoch  935  loss =  0\n",
            "epoch  936  loss =  0\n",
            "epoch  937  loss =  0\n",
            "epoch  938  loss =  0\n",
            "epoch  939  loss =  0\n",
            "epoch  940  loss =  0\n",
            "epoch  941  loss =  0\n",
            "epoch  942  loss =  0\n",
            "epoch  943  loss =  0\n",
            "epoch  944  loss =  0\n",
            "epoch  945  loss =  0\n",
            "epoch  946  loss =  0\n",
            "epoch  947  loss =  0\n",
            "epoch  948  loss =  0\n",
            "epoch  949  loss =  0\n",
            "epoch  950  loss =  0\n",
            "epoch  951  loss =  0\n",
            "epoch  952  loss =  0\n",
            "epoch  953  loss =  0\n",
            "epoch  954  loss =  0\n",
            "epoch  955  loss =  0\n",
            "epoch  956  loss =  0\n",
            "epoch  957  loss =  0\n",
            "epoch  958  loss =  0\n",
            "epoch  959  loss =  0\n",
            "epoch  960  loss =  0\n",
            "epoch  961  loss =  0\n",
            "epoch  962  loss =  0\n",
            "epoch  963  loss =  0\n",
            "epoch  964  loss =  0\n",
            "epoch  965  loss =  0\n",
            "epoch  966  loss =  0\n",
            "epoch  967  loss =  0\n",
            "epoch  968  loss =  0\n",
            "epoch  969  loss =  0\n",
            "epoch  970  loss =  0\n",
            "epoch  971  loss =  0\n",
            "epoch  972  loss =  0\n",
            "epoch  973  loss =  0\n",
            "epoch  974  loss =  0\n",
            "epoch  975  loss =  0\n",
            "epoch  976  loss =  0\n",
            "epoch  977  loss =  0\n",
            "epoch  978  loss =  0\n",
            "epoch  979  loss =  0\n",
            "epoch  980  loss =  0\n",
            "epoch  981  loss =  0\n",
            "epoch  982  loss =  0\n",
            "epoch  983  loss =  0\n",
            "epoch  984  loss =  0\n",
            "epoch  985  loss =  0\n",
            "epoch  986  loss =  0\n",
            "epoch  987  loss =  0\n",
            "epoch  988  loss =  0\n",
            "epoch  989  loss =  0\n",
            "epoch  990  loss =  0\n",
            "epoch  991  loss =  0\n",
            "epoch  992  loss =  0\n",
            "epoch  993  loss =  0\n",
            "epoch  994  loss =  0\n",
            "epoch  995  loss =  0\n",
            "epoch  996  loss =  0\n",
            "epoch  997  loss =  0\n",
            "epoch  998  loss =  0\n",
            "epoch  999  loss =  0\n",
            "['earth', 'revolves', 'around']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "wikipedia.set_lang(\"en\")\n",
        "article = wikipedia.page(\"Dragon\")\n",
        "corpus = article.content\n",
        "\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus.split(\".\")]\n",
        "\n",
        "window_sizes = [2, 5, 10]\n",
        "vector_dimensions = [50, 100, 200]\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "\n",
        "for window_size in window_sizes:\n",
        "    for vector_dimension in vector_dimensions:\n",
        "        for learning_rate in learning_rates:\n",
        "            model_name = f\"word2vec_w{window_size}_dim{vector_dimension}_lr{learning_rate}\"\n",
        "            print(f\"Training {model_name}...\")\n",
        "            model = Word2Vec(sentences=tokenized_corpus,\n",
        "                             window=window_size,\n",
        "                             vector_size=vector_dimension,\n",
        "                             alpha=learning_rate,\n",
        "                             min_count=1,\n",
        "                             sg=1)\n",
        "            model.save(f\"{model_name}.model\")\n",
        "            print(f\"{model_name} trained and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfueMr-gwX1O",
        "outputId": "b248f027-75cf-4c68-bdfc-43ccbfaa188b"
      },
      "id": "SfueMr-gwX1O",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training word2vec_w2_dim50_lr0.01...\n",
            "word2vec_w2_dim50_lr0.01 trained and saved.\n",
            "Training word2vec_w2_dim50_lr0.05...\n",
            "word2vec_w2_dim50_lr0.05 trained and saved.\n",
            "Training word2vec_w2_dim50_lr0.1...\n",
            "word2vec_w2_dim50_lr0.1 trained and saved.\n",
            "Training word2vec_w2_dim100_lr0.01...\n",
            "word2vec_w2_dim100_lr0.01 trained and saved.\n",
            "Training word2vec_w2_dim100_lr0.05...\n",
            "word2vec_w2_dim100_lr0.05 trained and saved.\n",
            "Training word2vec_w2_dim100_lr0.1...\n",
            "word2vec_w2_dim100_lr0.1 trained and saved.\n",
            "Training word2vec_w2_dim200_lr0.01...\n",
            "word2vec_w2_dim200_lr0.01 trained and saved.\n",
            "Training word2vec_w2_dim200_lr0.05...\n",
            "word2vec_w2_dim200_lr0.05 trained and saved.\n",
            "Training word2vec_w2_dim200_lr0.1...\n",
            "word2vec_w2_dim200_lr0.1 trained and saved.\n",
            "Training word2vec_w5_dim50_lr0.01...\n",
            "word2vec_w5_dim50_lr0.01 trained and saved.\n",
            "Training word2vec_w5_dim50_lr0.05...\n",
            "word2vec_w5_dim50_lr0.05 trained and saved.\n",
            "Training word2vec_w5_dim50_lr0.1...\n",
            "word2vec_w5_dim50_lr0.1 trained and saved.\n",
            "Training word2vec_w5_dim100_lr0.01...\n",
            "word2vec_w5_dim100_lr0.01 trained and saved.\n",
            "Training word2vec_w5_dim100_lr0.05...\n",
            "word2vec_w5_dim100_lr0.05 trained and saved.\n",
            "Training word2vec_w5_dim100_lr0.1...\n",
            "word2vec_w5_dim100_lr0.1 trained and saved.\n",
            "Training word2vec_w5_dim200_lr0.01...\n",
            "word2vec_w5_dim200_lr0.01 trained and saved.\n",
            "Training word2vec_w5_dim200_lr0.05...\n",
            "word2vec_w5_dim200_lr0.05 trained and saved.\n",
            "Training word2vec_w5_dim200_lr0.1...\n",
            "word2vec_w5_dim200_lr0.1 trained and saved.\n",
            "Training word2vec_w10_dim50_lr0.01...\n",
            "word2vec_w10_dim50_lr0.01 trained and saved.\n",
            "Training word2vec_w10_dim50_lr0.05...\n",
            "word2vec_w10_dim50_lr0.05 trained and saved.\n",
            "Training word2vec_w10_dim50_lr0.1...\n",
            "word2vec_w10_dim50_lr0.1 trained and saved.\n",
            "Training word2vec_w10_dim100_lr0.01...\n",
            "word2vec_w10_dim100_lr0.01 trained and saved.\n",
            "Training word2vec_w10_dim100_lr0.05...\n",
            "word2vec_w10_dim100_lr0.05 trained and saved.\n",
            "Training word2vec_w10_dim100_lr0.1...\n",
            "word2vec_w10_dim100_lr0.1 trained and saved.\n",
            "Training word2vec_w10_dim200_lr0.01...\n",
            "word2vec_w10_dim200_lr0.01 trained and saved.\n",
            "Training word2vec_w10_dim200_lr0.05...\n",
            "word2vec_w10_dim200_lr0.05 trained and saved.\n",
            "Training word2vec_w10_dim200_lr0.1...\n",
            "word2vec_w10_dim200_lr0.1 trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "models = {}\n",
        "window_sizes = [2, 5, 10]\n",
        "vector_dimensions = [50, 100, 200]\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "\n",
        "for window_size in window_sizes:\n",
        "    for vector_dimension in vector_dimensions:\n",
        "        for learning_rate in learning_rates:\n",
        "            model_name = f\"word2vec_w{window_size}_dim{vector_dimension}_lr{learning_rate}\"\n",
        "            model = Word2Vec.load(f\"{model_name}.model\")\n",
        "            models[model_name] = model\n",
        "\n",
        "word_pairs = [(\"king\", \"queen\"), (\"man\", \"woman\"), (\"sun\", \"moon\")]\n",
        "human_similarities = [4.1, 3.9, 3.8]\n",
        "\n",
        "model_similarities = {}\n",
        "for model_name, model in models.items():\n",
        "    model_similarities[model_name] = []\n",
        "    for word1, word2 in word_pairs:\n",
        "        similarity = model.wv.similarity(word1, word2)\n",
        "        model_similarities[model_name].append(similarity)\n",
        "\n",
        "correlations = {}\n",
        "for model_name, similarities in model_similarities.items():\n",
        "    spearman_corr, _ = spearmanr(human_similarities, similarities)\n",
        "    correlations[model_name] = spearman_corr\n",
        "\n",
        "for model_name, correlation in correlations.items():\n",
        "    print(f\"Correlation for {model_name}: {correlation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ticdRehxk1l",
        "outputId": "20698129-ec80-4704-c0d5-f9ac8af22cb0"
      },
      "id": "-ticdRehxk1l",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation for word2vec_w2_dim50_lr0.01: 0.5\n",
            "Correlation for word2vec_w2_dim50_lr0.05: 0.5\n",
            "Correlation for word2vec_w2_dim50_lr0.1: -0.5\n",
            "Correlation for word2vec_w2_dim100_lr0.01: 1.0\n",
            "Correlation for word2vec_w2_dim100_lr0.05: 0.5\n",
            "Correlation for word2vec_w2_dim100_lr0.1: -0.5\n",
            "Correlation for word2vec_w2_dim200_lr0.01: 0.5\n",
            "Correlation for word2vec_w2_dim200_lr0.05: 0.5\n",
            "Correlation for word2vec_w2_dim200_lr0.1: -0.5\n",
            "Correlation for word2vec_w5_dim50_lr0.01: 1.0\n",
            "Correlation for word2vec_w5_dim50_lr0.05: -0.5\n",
            "Correlation for word2vec_w5_dim50_lr0.1: -1.0\n",
            "Correlation for word2vec_w5_dim100_lr0.01: 0.5\n",
            "Correlation for word2vec_w5_dim100_lr0.05: -0.5\n",
            "Correlation for word2vec_w5_dim100_lr0.1: -1.0\n",
            "Correlation for word2vec_w5_dim200_lr0.01: 0.5\n",
            "Correlation for word2vec_w5_dim200_lr0.05: -0.5\n",
            "Correlation for word2vec_w5_dim200_lr0.1: -1.0\n",
            "Correlation for word2vec_w10_dim50_lr0.01: 0.5\n",
            "Correlation for word2vec_w10_dim50_lr0.05: -1.0\n",
            "Correlation for word2vec_w10_dim50_lr0.1: 0.5\n",
            "Correlation for word2vec_w10_dim100_lr0.01: 0.5\n",
            "Correlation for word2vec_w10_dim100_lr0.05: -1.0\n",
            "Correlation for word2vec_w10_dim100_lr0.1: 0.5\n",
            "Correlation for word2vec_w10_dim200_lr0.01: 0.5\n",
            "Correlation for word2vec_w10_dim200_lr0.05: -0.5\n",
            "Correlation for word2vec_w10_dim200_lr0.1: -0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b81d8e-c946-40df-a842-581816d28be9",
      "metadata": {
        "id": "47b81d8e-c946-40df-a842-581816d28be9"
      },
      "source": [
        "### Pregunta 2\n",
        "\n",
        "La factorización de matrices GloVe y PPMI son dos métodos utilizados en el procesamiento del lenguaje natural (NLP) para capturar relaciones semánticas entre palabras a partir de grandes corpus de texto. Ambos métodos se utilizan para generar representaciones vectoriales de palabras, lo que permite que las relaciones semánticas y sintácticas entre palabras se reflejen en el espacio vectorial.\n",
        "\n",
        "1 . GloVe (Global Vectors for Word Representation)\n",
        "GloVe es un modelo de aprendizaje no supervisado para obtener representaciones vectoriales de palabras. Fue desarrollado por investigadores de Stanford y combina elementos de dos enfoques principales en NLP: factorización de matrices y modelos basados en ventana de contexto (como word2vec). La idea principal detrás de GloVe es que las co-ocurrencias de palabras en un corpus pueden proporcionar información semántica valiosa.\n",
        "\n",
        "El modelo GloVe construye una matriz de co-ocurrencia global que tabula cuántas veces cada palabra aparece en el contexto de otras palabras dentro de un corpus. Luego, esta matriz se factoriza para reducir su dimensión, resultando en vectores de palabras más densos. El objetivo de la factorización es mantener la estructura semántica donde la distancia entre dos vectores de palabras refleje la similitud semántica entre las palabras correspondientes.\n",
        "\n",
        "2 . PPMI (Positive Pointwise Mutual Information)\n",
        "La PPMI es una técnica que se usa para calcular la asociación entre palabras basada en cuán frecuentemente aparecen juntas en comparación con cuán frecuentemente aparecen por separado. El \"Pointwise Mutual Information\" (PMI) de dos palabras mide la probabilidad de co-ocurrencia de las palabras en relación con las probabilidades de que cada palabra ocurra por sí sola. Sin embargo, PMI puede tener valores negativos, lo que puede ser problemático en algunos escenarios de modelado.\n",
        "\n",
        "Para solucionarlo, se utiliza PPMI, donde todos los valores negativos de PMI se reemplazan por cero, enfocándose solo en las asociaciones positivas. En NLP, la PPMI a menudo se usa como una técnica de pre-procesamiento para construir matrices de características que luego pueden ser factorizadas (similar a SVD en GloVe) para obtener representaciones vectoriales de palabras.\n",
        "\n",
        "Para implementar PPMI, primero construiremos una matriz de co-ocurrencia y luego convertiremos sus valores a PPMI. Usaremos numpy para las operaciones matemáticas y collections para construir la matriz de co-ocurrencia.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0261ec-a5a5-478a-acf4-d70a216e6479",
      "metadata": {
        "id": "8d0261ec-a5a5-478a-acf4-d70a216e6479"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import product\n",
        "\n",
        "# Función para construir la matriz de co-ocurrencia\n",
        "def co_occurrence_matrix(corpus, window_size=2):\n",
        "    vocab = set(corpus)\n",
        "    vocab = {word: i for i, word in enumerate(vocab)}\n",
        "    co_occurrences = defaultdict(Counter)\n",
        "\n",
        "    for i in range(len(corpus)):\n",
        "        token = corpus[i]\n",
        "        left = max(0, i-window_size)\n",
        "        right = min(len(corpus), i+window_size+1)\n",
        "\n",
        "        for j in range(left, right):\n",
        "            if i != j:\n",
        "                co_occurrences[token][corpus[j]] += 1\n",
        "\n",
        "    matrix = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    for token1, neighbors in co_occurrences.items():\n",
        "        for token2, count in neighbors.items():\n",
        "            matrix[vocab[token1], vocab[token2]] = count\n",
        "\n",
        "    return matrix, vocab\n",
        "\n",
        "# Función para calcular PPMI\n",
        "def ppmi_matrix(co_matrix, eps=1e-8):\n",
        "    total_sum = np.sum(co_matrix)\n",
        "    row_sums = np.sum(co_matrix, axis=1)\n",
        "    col_sums = np.sum(co_matrix, axis=0)\n",
        "\n",
        "    ppmi = np.maximum(\n",
        "        np.log((co_matrix * total_sum) / (row_sums[:, None] * col_sums[None, :] + eps)),\n",
        "        0\n",
        "    )\n",
        "    return ppmi\n",
        "\n",
        "# Ejemplo de uso\n",
        "corpus = \"the quick brown fox jumps over the lazy dog\".split()\n",
        "co_matrix, vocab = co_occurrence_matrix(corpus, window_size=2)\n",
        "ppmi = ppmi_matrix(co_matrix)\n",
        "\n",
        "print(ppmi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a29399-8799-4b5b-b251-175f1a5d5749",
      "metadata": {
        "id": "b3a29399-8799-4b5b-b251-175f1a5d5749"
      },
      "source": [
        "Implementar GloVe desde cero es más complejo debido a la optimización necesaria para ajustar los vectores de palabras. Sin embargo, puedes usar la biblioteca gensim, que tiene una implementación eficiente de GloVe. Utiliza el código realizado en clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e09653-32b6-4a06-b7fc-c48e011c5890",
      "metadata": {
        "id": "59e09653-32b6-4a06-b7fc-c48e011c5890"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Crear modelo Word2Vec con los mismos parámetros que GloVe\n",
        "modelo = Word2Vec(sentences=[corpus], vector_size=100, window=5, min_count=1, sg=0, workers=4, epochs=10)\n",
        "\n",
        "# Guardar y cargar el modelo (simulando una carga de GloVe)\n",
        "model.wv.save_word2vec_format('model.bin')\n",
        "glove_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
        "\n",
        "# Usar el modelo\n",
        "print(glove_model['fox'])  # Muestra el vector para la palabra \"fox\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee3b58c-fb61-4251-97df-74050d05c742",
      "metadata": {
        "id": "fee3b58c-fb61-4251-97df-74050d05c742"
      },
      "source": [
        "#### Ejercicios\n",
        "\n",
        "1. Modifica el tamaño de la ventana de contexto en la función co_occurrence_matrix para diferentes valores (por ejemplo, 1, 3, y 5) y observa cómo cambia la matriz PPMI resultante. Analiza cómo el tamaño de la ventana afecta las relaciones semánticas capturadas (1 punto).\n",
        "2. Implementa una función que identifique y muestre las palabras con mayor asociación (mayores valores PPMI) para una palabra dada. Utiliza esta función para explorar las relaciones semánticas de varias palabras clave en un corpus más grande (1 punto).\n",
        "3. Usa la biblioteca gensim para entrenar un modelo GloVe con un corpus más grande (por ejemplo, un conjunto de datos de reseñas de productos o artículos de noticias). Ajusta diferentes hiperparámetros como el tamaño del vector, el tamaño de la ventana, y el número de iteraciones. Evalúa los vectores de palabras resultantes en tareas de analogía y similaridad (1 punto).\n",
        "4. Realiza una comparación cualitativa y cuantitativa de las representaciones de palabras obtenidas a través de PPMI y GloVe. Considera aspectos como la capacidad de capturar sinónimos, antónimos y relaciones semánticas complejas. Discute en qué casos un método podría ser preferido sobre el otro (1 punto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f07e76-d8e0-44ff-bb9b-7bb860785832",
      "metadata": {
        "id": "54f07e76-d8e0-44ff-bb9b-7bb860785832"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6dbda1f-635d-4517-9739-c3f448c99b21",
      "metadata": {
        "id": "e6dbda1f-635d-4517-9739-c3f448c99b21"
      },
      "source": [
        "### Pregunta 3\n",
        "\n",
        "El desarrollo de modelos de redes neuronales recurrentes (RNNs) ha sido fundamental en el avance del procesamiento de secuencias de tiempo y lenguaje natural. Estos modelos son especialmente útiles en tareas como el reconocimiento de voz, la traducción automática y la generación de texto. Sin embargo, las RNNs básicas enfrentan desafíos significativos, como la desaparición y la explosión del gradiente, que obstaculizan su capacidad para aprender dependencias a largo plazo en los datos. Las unidades de memoria de largo y corto plazo (LSTM) y las unidades recurrentes con compuertas (GRU) se desarrollaron como soluciones a estos problemas, mejorando la capacidad de las redes para aprender de datos secuenciales a largo plazo.\n",
        "\n",
        "Una RNN básica procesa información secuencial mediante la actualización de su estado oculto con cada nuevo elemento de la secuencia. La naturaleza recurrente de estas redes les permite mantener una forma de 'memoria' sobre los elementos anteriores de la secuencia, utilizando la siguiente fórmula básica para actualizar el estado oculto en cada paso de tiempo $t$:\n",
        "\n",
        "$$\n",
        "h_t = \\sigma(W_{ih} x_t + W_{hh} h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "Donde $x_t$ es la entrada en el tiempo $t$, $h_t$ es el estado oculto en el tiempo $t$, $W_{ih}$ y $W_{hh}$ son los pesos de entrada y recurrentes, respectivamente, $b_h$ es el término de sesgo, y $\\sigma$ es una función de activación no lineal como tanh o ReLU.\n",
        "\n",
        "\n",
        "El entrenamiento de RNNs implica ajustar estos pesos mediante retropropagación a través del tiempo, lo que puede llevar a dos problemas principales:\n",
        "\n",
        "1. **Desaparición del gradiente:** Si los gradientes de los pesos son muy pequeños, disminuyen exponencialmente a medida que se propagan hacia atrás a través de cada paso de tiempo. Esto hace que sea difícil para la RNN aprender dependencias a largo plazo, ya que los gradientes se vuelven insignificantes para ajustar los pesos efectivamente en pasos de tiempo anteriores.\n",
        "\n",
        "2. **Explosión del gradiente:** En contraste, si los gradientes son demasiado grandes, pueden crecer exponencialmente durante la retropropagación, lo que lleva a actualizaciones de peso grandes e inestables, y por ende, a un modelo que diverge y no aprende de manera efectiva.\n",
        "\n",
        "#### Unidad de memoria de largo y corto plazo (LSTM)\n",
        "\n",
        "Para abordar estos problemas, se introdujeron las LSTMs, que incorporan un diseño más complejo que permite controlar el flujo de información. Las LSTMs utilizan varias \"puertas\" para regular tanto el almacenamiento como la eliminación de información en el estado de la celda:\n",
        "\n",
        "- **Puerta de olvido $(f_t)$** decide qué parte de la información anterior se mantiene:\n",
        "  $$\n",
        "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "  $$\n",
        "\n",
        "- **Puerta de entrada ($i_t$) y candidato de celda ($\\tilde{c}_t$)** deciden qué nueva información se añade al estado de la celda:\n",
        "\n",
        "  $$\n",
        "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "  $$\n",
        "  $$\n",
        "  \\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
        "  $$\n",
        "\n",
        "- **Actualización del estado de la celda ($c_t$)** combina la información antigua y nueva:\n",
        "  $$\n",
        "  c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t\n",
        "  $$\n",
        "\n",
        "- **Puerta de salida ($o_t$)** y el estado oculto resultante ($h_t$) que determina qué parte del estado de la celda afectará la salida:\n",
        "  $$\n",
        "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "  $$\n",
        "  $$\n",
        "  h_t = o_t \\ast \\tanh(c_t)\n",
        "  $$\n",
        "\n",
        "#### Unidad recurrente compuerta (GRU)\n",
        "\n",
        "Las GRUs simplifican la arquitectura de las LSTMs combinando las puertas de entrada y olvido en una sola puerta de actualización y omitiendo el uso de un estado de celda separado:\n",
        "\n",
        "- **Puerta de actualización ($z_t$)** decide cuánto del estado anterior se debe mantener:\n",
        "  $$\n",
        "  z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
        "  $$\n",
        "\n",
        "- **Puerta de reinicio ($r_t$)** decide cuánto del pasado se debe olvidar antes de calcular el nuevo candidato de estado:\n",
        "  $$\n",
        "  r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
        "  $$\n",
        "\n",
        "- **Candidato de estado oculto ($\\tilde{h}_t$)** y la actualización del estado oculto:\n",
        "  $$\n",
        "  \\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\ast h_{t-1}, x_t] + b_h)\n",
        "  $$\n",
        "  \n",
        "  $$\n",
        "  h_t = (1 - z_t) \\ast h_{t-1} + z_t \\ast \\tilde{h}_t\n",
        "  $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead46557-86c0-4b63-a6cd-78fafa796d30",
      "metadata": {
        "id": "ead46557-86c0-4b63-a6cd-78fafa796d30"
      },
      "source": [
        "#### Ejercicios\n",
        "\n",
        "1. ¿Qué papel juegan los reguladores como dropout o L2 regularization específicamente en el contexto de RNNs y LSTM para evitar el sobreajuste en tareas de modelado de lenguaje? (1 punto)\n",
        "2. Considerando la complejidad computacional de BPTT, ¿cuáles son las limitaciones prácticas cuando se usa con RNNs en secuencias muy largas? ¿Cómo podrías mitigar estos problemas en un entorno de producción? (1 punto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Los reguladores como dropout o L2 regularization tienen un papel muy importante en el contexto de RNNs y LSTM ya que a través de ellos es posible evitar el sobreajuste en tareas de modelado de lenguaje. Mediante estas técnicas uno es capaz de reducir la complejidad de los modelos y evitar que se ajusten demasiado a los datos de entrenamiento, lo cual podría causar que se tenga un rendimiento bastante pobre a la hora de probarlo con datos nuevos.\n",
        "\n",
        "2. Al usar BPTT con RNNs en secuencias muy largas, hay limitaciones prácticas debido a la complejidad computacional que esta posee. Esto puede resultar en un tiempo de entrenamiento  muy prolongado y un uso excesivo de memoria, ralentizando aún más el proceso. Para mitigar estos problemas en un entorno de producción, se pueden utilizar técnicas de optimización como el truncamiento de BPTT o el uso de técnicas de mini-batch. También se puede considerar el uso de GPUs o el entrenamiento en paralelo para asi tener más formas de acelerar el proceso de entrenamiento."
      ],
      "metadata": {
        "id": "f3Rq0QGsiE1O"
      },
      "id": "f3Rq0QGsiE1O"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "78c0b968-53e3-4fd5-bff8-0618b4d8ef92",
      "metadata": {
        "id": "78c0b968-53e3-4fd5-bff8-0618b4d8ef92"
      },
      "outputs": [],
      "source": [
        "## Parte 3\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = torch.tanh(self.input_to_hidden(combined))\n",
        "        return hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Gates\n",
        "        self.input_to_inputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_to_forgetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_to_outputgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_to_cellgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "\n",
        "        # Calculate gates\n",
        "        input_gate = torch.sigmoid(self.input_to_inputgate(combined))\n",
        "        forget_gate = torch.sigmoid(self.input_to_forgetgate(combined))\n",
        "        output_gate = torch.sigmoid(self.input_to_outputgate(combined))\n",
        "        cell_gate = torch.tanh(self.input_to_cellgate(combined))\n",
        "\n",
        "        # Update cell state\n",
        "        cell = forget_gate * cell + input_gate * cell_gate\n",
        "        hidden = output_gate * torch.tanh(cell)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_to_updategate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_to_resetgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_to_newgate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "\n",
        "        # Calculate gates\n",
        "        update_gate = torch.sigmoid(self.input_to_updategate(combined))\n",
        "        reset_gate = torch.sigmoid(self.input_to_resetgate(combined))\n",
        "        new_gate = torch.tanh(self.input_to_newgate(combined * reset_gate))\n",
        "\n",
        "        # Update hidden state\n",
        "        hidden = update_gate * hidden + (1 - update_gate) * new_gate\n",
        "\n",
        "        return hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden.store)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2a9703-79cb-4435-b9bc-239985f6c3f2",
      "metadata": {
        "id": "1c2a9703-79cb-4435-b9bc-239985f6c3f2"
      },
      "source": [
        "Extiende la implementación de LSTM para incluir embeddings de palabras y una capa de clasificación, y entrenar el modelo en una tarea de predicción de la siguiente palabra en secuencias de texto (3 puntos).\n",
        "\n",
        "- Agrega una capa de embedding al modelo LSTM para procesar entradas de texto.\n",
        "- Incluye una capa de salida que mapee el estado oculto a las predicciones de palabras.\n",
        "- Implementa una función de pérdida adecuada para la clasificación de palabras.\n",
        "- Preprocesa  un corpus de texto grande (utiliza los datos dados en clase por ejemplo) para convertir texto a índices utilizando un vocabulario predefinido.\n",
        "- Genera datos de entrenamiento como pares de secuencias de entrada y palabras objetivo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "seJxkSGazMlQ"
      },
      "id": "seJxkSGazMlQ",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMWithEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(LSTMWithEmbedding, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = LSTM(embedding_dim, hidden_size)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embedded = self.embedding(input)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded, hidden, cell)\n",
        "        output = self.output_layer(lstm_out)\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)\n"
      ],
      "metadata": {
        "id": "hVqtOJAPrfRs"
      },
      "id": "hVqtOJAPrfRs",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_data, word_to_index, context_size):\n",
        "        self.text_data = text_data\n",
        "        self.word_to_index = word_to_index\n",
        "        self.context_size = context_size\n",
        "        self.data = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        for i in range(len(self.text_data) - self.context_size):\n",
        "            context = [self.word_to_index[word] for word in self.text_data[i:i+self.context_size]]\n",
        "            target = self.word_to_index[self.text_data[i+self.context_size]]\n",
        "            data.append((context, target))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(target)"
      ],
      "metadata": {
        "id": "AaAHx9KDzSuE"
      },
      "id": "AaAHx9KDzSuE",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text_corpus, vocab):\n",
        "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "    indexed_corpus = [word_to_index[word] for word in text_corpus if word in vocab]\n",
        "    return indexed_corpus, word_to_index\n",
        "\n",
        "def generate_training_data(indexed_corpus, context_size):\n",
        "    training_data = []\n",
        "    for i in range(len(indexed_corpus) - context_size):\n",
        "        context = indexed_corpus[i:i+context_size]\n",
        "        target = indexed_corpus[i+context_size]\n",
        "        training_data.append((context, target))\n",
        "    return training_data\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "XMci069QzXHw"
      },
      "id": "XMci069QzXHw",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2da3c614-442c-44d5-92bf-4b61280d9c0e",
      "metadata": {
        "id": "2da3c614-442c-44d5-92bf-4b61280d9c0e"
      },
      "source": [
        "Realiza un análisis de sensibilidad de los hiperparámetros en modelos LSTM y GRU para entender su impacto en la capacidad de aprendizaje de dependencias a largo plazo en textos (3 puntos)\n",
        "\n",
        "- Selecciona un corpus de texto y prepara datos para el entrenamiento de modelos de lenguaje basados en LSTM y GRU.\n",
        "- Experimenta con diferentes valores para los hiperparámetros como el tamaño de las puertas, la tasa de aprendizaje, el tamaño del estado oculto y la longitud de BPTT.\n",
        "- Utiliza técnicas como validación cruzada para evaluar el impacto de estos cambios en la precisión del modelo y en su capacidad para generar texto coherente.\n",
        "- Analiza cómo la modificación de los parámetros de las puertas y la longitud de BPTT afecta la estabilidad del entrenamiento y la convergencia del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ca146e-2886-4112-b00a-f3ae28d46139",
      "metadata": {
        "id": "d3ca146e-2886-4112-b00a-f3ae28d46139"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dcf1093-7292-469a-b041-3c7120f4ee45",
      "metadata": {
        "id": "6dcf1093-7292-469a-b041-3c7120f4ee45"
      },
      "source": [
        "### Pregunta 4\n",
        "El script proporcionado es un ejemplo completo de cómo implementar un modelo de red neuronal recurrente (RNN) utilizando PyTorch para generar texto de manera automática."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01cc564-c049-4af8-8153-26e0f5816d2c",
      "metadata": {
        "id": "b01cc564-c049-4af8-8153-26e0f5816d2c"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías necesarias para trabajar con tensores y redes neuronales.\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "# Datos de entrada: una lista de frases.\n",
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Creación de un conjunto de caracteres únicos presentes en las frases.\n",
        "chars = set(''.join(text))\n",
        "# Creación de un diccionario que mapea cada caracter a un índice único.\n",
        "int2char = dict(enumerate(chars))\n",
        "# Creación de un diccionario inverso que mapea cada índice a su caracter correspondiente.\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "\n",
        "# Determinación de la longitud máxima de las frases para normalizar la longitud de todas.\n",
        "maxlen = len(max(text, key=len))\n",
        "print(\"La longitud mayor tiene {} caracteres\".format(maxlen))\n",
        "\n",
        "# Añadir espacios a las frases más cortas para igualar la longitud máxima.\n",
        "for i in range(len(text)):\n",
        "  while len(text[i])<maxlen:\n",
        "    text[i] += ' '\n",
        "\n",
        "# Inicialización de listas para secuencias de entrada y objetivo.\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "# Creación de secuencias de entrada y objetivo.\n",
        "for i in range(len(text)):\n",
        "    input_seq.append(text[i][:-1])\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Secuencia entrada: {}\\nSecuencia objetivo: {}\".format(input_seq[i], target_seq[i]))\n",
        "\n",
        "# Conversión de caracteres a índices para procesamiento numérico.\n",
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
        "\n",
        "# Definición de tamaños para la codificación one-hot.\n",
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "# Función para codificar las secuencias en formato one-hot.\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "          features[i, u, sequence[i][u]] = 1\n",
        "    return features\n",
        "\n",
        "# Aplicación de la codificación one-hot a las secuencias de entrada.\n",
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Forma de entrada: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
        "\n",
        "# Conversión de las secuencias de entrada a tensores de PyTorch.\n",
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)\n",
        "\n",
        "# Chequeo de disponibilidad de GPU y selección del dispositivo (GPU o CPU).\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU es disponible\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU no disponible, CPU es usada\")\n",
        "\n",
        "# Definición de la clase del modelo RNN.\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Capa RNN que toma entradas y retorna la salida y un estado oculto.\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Capa lineal que procesa la salida del RNN.\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Inicialización del estado oculto a cero.\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden\n",
        "\n",
        "# Instancia del modelo con parámetros específicos.\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "model.to(device)\n",
        "\n",
        "# Definición de hiperparámetros para el entrenamiento.\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Configuración de la función de pérdida y el optimizador.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Bucle de entrenamiento del modelo.\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  optimizer.zero_grad()\n",
        "  input_seq = input_seq.to(device)\n",
        "  output, hidden = model(input_seq)\n",
        "  loss = criterion(output, target_seq.view(-1).long())\n",
        "  loss.backward() # Realización de backpropagation y cálculo de gradientes.\n",
        "  optimizer.step() # Actualización de los pesos del modelo.\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "    print(\"Loss: {:.4f}\".format(loss.item()))\n",
        "\n",
        "# Funciones para predicción y generación de texto basadas en el modelo entrenado.\n",
        "def predict(model, character):\n",
        "  character = np.array([[char2int[c] for c in character]])\n",
        "  character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "  character = torch.from_numpy(character)\n",
        "  character.to(device)\n",
        "\n",
        "  out, hidden = model(character)\n",
        "\n",
        "  prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "  char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "  return int2char[char_ind], hidden\n",
        "\n",
        "def sample(model, out_len, start='hey'):\n",
        "  model.eval()\n",
        "  start = start.lower()\n",
        "  chars = [ch for ch in start]\n",
        "  size = out_len - len(chars)\n",
        "  for ii in range(size):\n",
        "    char, h = predict(model, chars)\n",
        "    chars.append(char)\n",
        "\n",
        "  return ''.join(chars)\n",
        "\n",
        "# Ejemplo de uso de la función de generación de texto.\n",
        "sample(model, 15, 'good')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c017567c-c7c0-4d27-81f1-1abeee08518f",
      "metadata": {
        "id": "c017567c-c7c0-4d27-81f1-1abeee08518f"
      },
      "source": [
        "#### Ejercicios:\n",
        "\n",
        "1. Modifica el modelo existente para que funcione como un autoencoder. Esto implica que el modelo debe aprender a codificar una secuencia de entrada en un vector de características (estado oculto) y luego decodificar ese vector de vuelta a la secuencia original (1.5 puntos).\n",
        "     - Implementa las capas de codificación y decodificación dentro del mismo modelo.\n",
        "     - Experimenta  con diferentes estructuras como LSTM para mejorar la retención de información.\n",
        "     - Mide la calidad de la reconstrucción del texto y la eficiencia de compresión.\n",
        "\n",
        "2. Utiliza el modelo RNN actual y modifícalo para introducir secuencias más largas. Monitoriza los gradientes durante el entrenamiento para detectar signos de desaparición o explosión. (1.5 puntos)\n",
        "    - Implementa el  clipping de gradiente para prevenir la explosión del gradiente.\n",
        "    - Reemplaza la RNN por LSTM para abordar la desaparición del gradiente.\n",
        "    - Utiliza técnicas de visualización para observar la magnitud de los gradientes a lo largo de varias épocas.\n",
        "3. Implementa el dropout en las capas recurrentes y comparar los resultados. (1 punto)\n",
        "\n",
        "    - Ajusta el parámetro de weight decay en el optimizador y observar el efecto sobre el overfitting.\n",
        "    - Aplica early stopping basado en la validación del loss para detener el entrenamiento antes de que el modelo comience a sobreajustarse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff7c8ab-52f4-4a1f-a66d-7f3f4c32e486",
      "metadata": {
        "id": "2ff7c8ab-52f4-4a1f-a66d-7f3f4c32e486"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}